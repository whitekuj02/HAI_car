{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 09:31:25.487705: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-12 09:31:25.496100: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749688285.504876 2368790 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749688285.507514 2368790 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-12 09:31:25.517112: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.7'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, ConcatDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import timm\n",
    "from sklearn.metrics import log_loss\n",
    "import random\n",
    "from torch.optim.swa_utils import AveragedModel, update_bn, SWALR\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "from torchcam.methods import SmoothGradCAMpp  # or GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# seed_everything(42) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"/home/aicontest/HAI_car/data/test\"\n",
    "train_path = \"/home/aicontest/HAI_car/data/train\"\n",
    "train_class = [i for i in os.listdir(train_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TEST_00000.jpg',\n",
       " 'TEST_00001.jpg',\n",
       " 'TEST_00002.jpg',\n",
       " 'TEST_00003.jpg',\n",
       " 'TEST_00004.jpg',\n",
       " 'TEST_00005.jpg',\n",
       " 'TEST_00006.jpg',\n",
       " 'TEST_00007.jpg',\n",
       " 'TEST_00008.jpg',\n",
       " 'TEST_00009.jpg',\n",
       " 'TEST_00010.jpg',\n",
       " 'TEST_00011.jpg',\n",
       " 'TEST_00012.jpg',\n",
       " 'TEST_00013.jpg',\n",
       " 'TEST_00014.jpg',\n",
       " 'TEST_00015.jpg',\n",
       " 'TEST_00016.jpg',\n",
       " 'TEST_00017.jpg',\n",
       " 'TEST_00018.jpg',\n",
       " 'TEST_00019.jpg',\n",
       " 'TEST_00020.jpg',\n",
       " 'TEST_00021.jpg',\n",
       " 'TEST_00022.jpg',\n",
       " 'TEST_00023.jpg',\n",
       " 'TEST_00024.jpg',\n",
       " 'TEST_00025.jpg',\n",
       " 'TEST_00026.jpg',\n",
       " 'TEST_00027.jpg',\n",
       " 'TEST_00028.jpg',\n",
       " 'TEST_00029.jpg',\n",
       " 'TEST_00030.jpg',\n",
       " 'TEST_00031.jpg',\n",
       " 'TEST_00032.jpg',\n",
       " 'TEST_00033.jpg',\n",
       " 'TEST_00034.jpg',\n",
       " 'TEST_00035.jpg',\n",
       " 'TEST_00036.jpg',\n",
       " 'TEST_00037.jpg',\n",
       " 'TEST_00038.jpg',\n",
       " 'TEST_00039.jpg',\n",
       " 'TEST_00040.jpg',\n",
       " 'TEST_00041.jpg',\n",
       " 'TEST_00042.jpg',\n",
       " 'TEST_00043.jpg',\n",
       " 'TEST_00044.jpg',\n",
       " 'TEST_00045.jpg',\n",
       " 'TEST_00046.jpg',\n",
       " 'TEST_00047.jpg',\n",
       " 'TEST_00048.jpg',\n",
       " 'TEST_00049.jpg',\n",
       " 'TEST_00050.jpg',\n",
       " 'TEST_00051.jpg',\n",
       " 'TEST_00052.jpg',\n",
       " 'TEST_00053.jpg',\n",
       " 'TEST_00054.jpg',\n",
       " 'TEST_00055.jpg',\n",
       " 'TEST_00056.jpg',\n",
       " 'TEST_00057.jpg',\n",
       " 'TEST_00058.jpg',\n",
       " 'TEST_00059.jpg',\n",
       " 'TEST_00060.jpg',\n",
       " 'TEST_00061.jpg',\n",
       " 'TEST_00062.jpg',\n",
       " 'TEST_00063.jpg',\n",
       " 'TEST_00064.jpg',\n",
       " 'TEST_00065.jpg',\n",
       " 'TEST_00066.jpg',\n",
       " 'TEST_00067.jpg',\n",
       " 'TEST_00068.jpg',\n",
       " 'TEST_00069.jpg',\n",
       " 'TEST_00070.jpg',\n",
       " 'TEST_00071.jpg',\n",
       " 'TEST_00072.jpg',\n",
       " 'TEST_00073.jpg',\n",
       " 'TEST_00074.jpg',\n",
       " 'TEST_00075.jpg',\n",
       " 'TEST_00076.jpg',\n",
       " 'TEST_00077.jpg',\n",
       " 'TEST_00078.jpg',\n",
       " 'TEST_00079.jpg',\n",
       " 'TEST_00080.jpg',\n",
       " 'TEST_00081.jpg',\n",
       " 'TEST_00082.jpg',\n",
       " 'TEST_00083.jpg',\n",
       " 'TEST_00084.jpg',\n",
       " 'TEST_00085.jpg',\n",
       " 'TEST_00086.jpg',\n",
       " 'TEST_00087.jpg',\n",
       " 'TEST_00088.jpg',\n",
       " 'TEST_00089.jpg',\n",
       " 'TEST_00090.jpg',\n",
       " 'TEST_00091.jpg',\n",
       " 'TEST_00092.jpg',\n",
       " 'TEST_00093.jpg',\n",
       " 'TEST_00094.jpg',\n",
       " 'TEST_00095.jpg',\n",
       " 'TEST_00096.jpg',\n",
       " 'TEST_00097.jpg',\n",
       " 'TEST_00098.jpg',\n",
       " 'TEST_00099.jpg',\n",
       " 'TEST_00100.jpg',\n",
       " 'TEST_00101.jpg',\n",
       " 'TEST_00102.jpg',\n",
       " 'TEST_00103.jpg',\n",
       " 'TEST_00104.jpg',\n",
       " 'TEST_00105.jpg',\n",
       " 'TEST_00106.jpg',\n",
       " 'TEST_00107.jpg',\n",
       " 'TEST_00108.jpg',\n",
       " 'TEST_00109.jpg',\n",
       " 'TEST_00110.jpg',\n",
       " 'TEST_00111.jpg',\n",
       " 'TEST_00112.jpg',\n",
       " 'TEST_00113.jpg',\n",
       " 'TEST_00114.jpg',\n",
       " 'TEST_00115.jpg',\n",
       " 'TEST_00116.jpg',\n",
       " 'TEST_00117.jpg',\n",
       " 'TEST_00118.jpg',\n",
       " 'TEST_00119.jpg',\n",
       " 'TEST_00120.jpg',\n",
       " 'TEST_00121.jpg',\n",
       " 'TEST_00122.jpg',\n",
       " 'TEST_00123.jpg',\n",
       " 'TEST_00124.jpg',\n",
       " 'TEST_00125.jpg',\n",
       " 'TEST_00126.jpg',\n",
       " 'TEST_00127.jpg',\n",
       " 'TEST_00128.jpg',\n",
       " 'TEST_00129.jpg',\n",
       " 'TEST_00130.jpg',\n",
       " 'TEST_00131.jpg',\n",
       " 'TEST_00132.jpg',\n",
       " 'TEST_00133.jpg',\n",
       " 'TEST_00134.jpg',\n",
       " 'TEST_00135.jpg',\n",
       " 'TEST_00136.jpg',\n",
       " 'TEST_00137.jpg',\n",
       " 'TEST_00138.jpg',\n",
       " 'TEST_00139.jpg',\n",
       " 'TEST_00140.jpg',\n",
       " 'TEST_00141.jpg',\n",
       " 'TEST_00142.jpg',\n",
       " 'TEST_00143.jpg',\n",
       " 'TEST_00144.jpg',\n",
       " 'TEST_00145.jpg',\n",
       " 'TEST_00146.jpg',\n",
       " 'TEST_00147.jpg',\n",
       " 'TEST_00148.jpg',\n",
       " 'TEST_00149.jpg',\n",
       " 'TEST_00150.jpg',\n",
       " 'TEST_00151.jpg',\n",
       " 'TEST_00152.jpg',\n",
       " 'TEST_00153.jpg',\n",
       " 'TEST_00154.jpg',\n",
       " 'TEST_00155.jpg',\n",
       " 'TEST_00156.jpg',\n",
       " 'TEST_00157.jpg',\n",
       " 'TEST_00158.jpg',\n",
       " 'TEST_00159.jpg',\n",
       " 'TEST_00160.jpg',\n",
       " 'TEST_00161.jpg',\n",
       " 'TEST_00162.jpg',\n",
       " 'TEST_00163.jpg',\n",
       " 'TEST_00164.jpg',\n",
       " 'TEST_00165.jpg',\n",
       " 'TEST_00166.jpg',\n",
       " 'TEST_00167.jpg',\n",
       " 'TEST_00168.jpg',\n",
       " 'TEST_00169.jpg',\n",
       " 'TEST_00170.jpg',\n",
       " 'TEST_00171.jpg',\n",
       " 'TEST_00172.jpg',\n",
       " 'TEST_00173.jpg',\n",
       " 'TEST_00174.jpg',\n",
       " 'TEST_00175.jpg',\n",
       " 'TEST_00176.jpg',\n",
       " 'TEST_00177.jpg',\n",
       " 'TEST_00178.jpg',\n",
       " 'TEST_00179.jpg',\n",
       " 'TEST_00180.jpg',\n",
       " 'TEST_00181.jpg',\n",
       " 'TEST_00182.jpg',\n",
       " 'TEST_00183.jpg',\n",
       " 'TEST_00184.jpg',\n",
       " 'TEST_00185.jpg',\n",
       " 'TEST_00186.jpg',\n",
       " 'TEST_00187.jpg',\n",
       " 'TEST_00188.jpg',\n",
       " 'TEST_00189.jpg',\n",
       " 'TEST_00190.jpg',\n",
       " 'TEST_00191.jpg',\n",
       " 'TEST_00192.jpg',\n",
       " 'TEST_00193.jpg',\n",
       " 'TEST_00194.jpg',\n",
       " 'TEST_00195.jpg',\n",
       " 'TEST_00196.jpg',\n",
       " 'TEST_00197.jpg',\n",
       " 'TEST_00198.jpg',\n",
       " 'TEST_00199.jpg',\n",
       " 'TEST_00200.jpg',\n",
       " 'TEST_00201.jpg',\n",
       " 'TEST_00202.jpg',\n",
       " 'TEST_00203.jpg',\n",
       " 'TEST_00204.jpg',\n",
       " 'TEST_00205.jpg',\n",
       " 'TEST_00206.jpg',\n",
       " 'TEST_00207.jpg',\n",
       " 'TEST_00208.jpg',\n",
       " 'TEST_00209.jpg',\n",
       " 'TEST_00210.jpg',\n",
       " 'TEST_00211.jpg',\n",
       " 'TEST_00212.jpg',\n",
       " 'TEST_00213.jpg',\n",
       " 'TEST_00214.jpg',\n",
       " 'TEST_00215.jpg',\n",
       " 'TEST_00216.jpg',\n",
       " 'TEST_00217.jpg',\n",
       " 'TEST_00218.jpg',\n",
       " 'TEST_00219.jpg',\n",
       " 'TEST_00220.jpg',\n",
       " 'TEST_00221.jpg',\n",
       " 'TEST_00222.jpg',\n",
       " 'TEST_00223.jpg',\n",
       " 'TEST_00224.jpg',\n",
       " 'TEST_00225.jpg',\n",
       " 'TEST_00226.jpg',\n",
       " 'TEST_00227.jpg',\n",
       " 'TEST_00228.jpg',\n",
       " 'TEST_00229.jpg',\n",
       " 'TEST_00230.jpg',\n",
       " 'TEST_00231.jpg',\n",
       " 'TEST_00232.jpg',\n",
       " 'TEST_00233.jpg',\n",
       " 'TEST_00234.jpg',\n",
       " 'TEST_00235.jpg',\n",
       " 'TEST_00236.jpg',\n",
       " 'TEST_00237.jpg',\n",
       " 'TEST_00238.jpg',\n",
       " 'TEST_00239.jpg',\n",
       " 'TEST_00240.jpg',\n",
       " 'TEST_00241.jpg',\n",
       " 'TEST_00242.jpg',\n",
       " 'TEST_00243.jpg',\n",
       " 'TEST_00244.jpg',\n",
       " 'TEST_00245.jpg',\n",
       " 'TEST_00246.jpg',\n",
       " 'TEST_00247.jpg',\n",
       " 'TEST_00248.jpg',\n",
       " 'TEST_00249.jpg',\n",
       " 'TEST_00250.jpg',\n",
       " 'TEST_00251.jpg',\n",
       " 'TEST_00252.jpg',\n",
       " 'TEST_00253.jpg',\n",
       " 'TEST_00254.jpg',\n",
       " 'TEST_00255.jpg',\n",
       " 'TEST_00256.jpg',\n",
       " 'TEST_00257.jpg',\n",
       " 'TEST_00258.jpg',\n",
       " 'TEST_00259.jpg',\n",
       " 'TEST_00260.jpg',\n",
       " 'TEST_00261.jpg',\n",
       " 'TEST_00262.jpg',\n",
       " 'TEST_00263.jpg',\n",
       " 'TEST_00264.jpg',\n",
       " 'TEST_00265.jpg',\n",
       " 'TEST_00266.jpg',\n",
       " 'TEST_00267.jpg',\n",
       " 'TEST_00268.jpg',\n",
       " 'TEST_00269.jpg',\n",
       " 'TEST_00270.jpg',\n",
       " 'TEST_00271.jpg',\n",
       " 'TEST_00272.jpg',\n",
       " 'TEST_00273.jpg',\n",
       " 'TEST_00274.jpg',\n",
       " 'TEST_00275.jpg',\n",
       " 'TEST_00276.jpg',\n",
       " 'TEST_00277.jpg',\n",
       " 'TEST_00278.jpg',\n",
       " 'TEST_00279.jpg',\n",
       " 'TEST_00280.jpg',\n",
       " 'TEST_00281.jpg',\n",
       " 'TEST_00282.jpg',\n",
       " 'TEST_00283.jpg',\n",
       " 'TEST_00284.jpg',\n",
       " 'TEST_00285.jpg',\n",
       " 'TEST_00286.jpg',\n",
       " 'TEST_00287.jpg',\n",
       " 'TEST_00288.jpg',\n",
       " 'TEST_00289.jpg',\n",
       " 'TEST_00290.jpg',\n",
       " 'TEST_00291.jpg',\n",
       " 'TEST_00292.jpg',\n",
       " 'TEST_00293.jpg',\n",
       " 'TEST_00294.jpg',\n",
       " 'TEST_00295.jpg',\n",
       " 'TEST_00296.jpg',\n",
       " 'TEST_00297.jpg',\n",
       " 'TEST_00298.jpg',\n",
       " 'TEST_00299.jpg',\n",
       " 'TEST_00300.jpg',\n",
       " 'TEST_00301.jpg',\n",
       " 'TEST_00302.jpg',\n",
       " 'TEST_00303.jpg',\n",
       " 'TEST_00304.jpg',\n",
       " 'TEST_00305.jpg',\n",
       " 'TEST_00306.jpg',\n",
       " 'TEST_00307.jpg',\n",
       " 'TEST_00308.jpg',\n",
       " 'TEST_00309.jpg',\n",
       " 'TEST_00310.jpg',\n",
       " 'TEST_00311.jpg',\n",
       " 'TEST_00312.jpg',\n",
       " 'TEST_00313.jpg',\n",
       " 'TEST_00314.jpg',\n",
       " 'TEST_00315.jpg',\n",
       " 'TEST_00316.jpg',\n",
       " 'TEST_00317.jpg',\n",
       " 'TEST_00318.jpg',\n",
       " 'TEST_00319.jpg',\n",
       " 'TEST_00320.jpg',\n",
       " 'TEST_00321.jpg',\n",
       " 'TEST_00322.jpg',\n",
       " 'TEST_00323.jpg',\n",
       " 'TEST_00324.jpg',\n",
       " 'TEST_00325.jpg',\n",
       " 'TEST_00326.jpg',\n",
       " 'TEST_00327.jpg',\n",
       " 'TEST_00328.jpg',\n",
       " 'TEST_00329.jpg',\n",
       " 'TEST_00330.jpg',\n",
       " 'TEST_00331.jpg',\n",
       " 'TEST_00332.jpg',\n",
       " 'TEST_00333.jpg',\n",
       " 'TEST_00334.jpg',\n",
       " 'TEST_00335.jpg',\n",
       " 'TEST_00336.jpg',\n",
       " 'TEST_00337.jpg',\n",
       " 'TEST_00338.jpg',\n",
       " 'TEST_00339.jpg',\n",
       " 'TEST_00340.jpg',\n",
       " 'TEST_00341.jpg',\n",
       " 'TEST_00342.jpg',\n",
       " 'TEST_00343.jpg',\n",
       " 'TEST_00344.jpg',\n",
       " 'TEST_00345.jpg',\n",
       " 'TEST_00346.jpg',\n",
       " 'TEST_00347.jpg',\n",
       " 'TEST_00348.jpg',\n",
       " 'TEST_00349.jpg',\n",
       " 'TEST_00350.jpg',\n",
       " 'TEST_00351.jpg',\n",
       " 'TEST_00352.jpg',\n",
       " 'TEST_00353.jpg',\n",
       " 'TEST_00354.jpg',\n",
       " 'TEST_00355.jpg',\n",
       " 'TEST_00356.jpg',\n",
       " 'TEST_00357.jpg',\n",
       " 'TEST_00358.jpg',\n",
       " 'TEST_00359.jpg',\n",
       " 'TEST_00360.jpg',\n",
       " 'TEST_00361.jpg',\n",
       " 'TEST_00362.jpg',\n",
       " 'TEST_00363.jpg',\n",
       " 'TEST_00364.jpg',\n",
       " 'TEST_00365.jpg',\n",
       " 'TEST_00366.jpg',\n",
       " 'TEST_00367.jpg',\n",
       " 'TEST_00368.jpg',\n",
       " 'TEST_00369.jpg',\n",
       " 'TEST_00370.jpg',\n",
       " 'TEST_00371.jpg',\n",
       " 'TEST_00372.jpg',\n",
       " 'TEST_00373.jpg',\n",
       " 'TEST_00374.jpg',\n",
       " 'TEST_00375.jpg',\n",
       " 'TEST_00376.jpg',\n",
       " 'TEST_00377.jpg',\n",
       " 'TEST_00378.jpg',\n",
       " 'TEST_00379.jpg',\n",
       " 'TEST_00380.jpg',\n",
       " 'TEST_00381.jpg',\n",
       " 'TEST_00382.jpg',\n",
       " 'TEST_00383.jpg',\n",
       " 'TEST_00384.jpg',\n",
       " 'TEST_00385.jpg',\n",
       " 'TEST_00386.jpg',\n",
       " 'TEST_00387.jpg',\n",
       " 'TEST_00388.jpg',\n",
       " 'TEST_00389.jpg',\n",
       " 'TEST_00390.jpg',\n",
       " 'TEST_00391.jpg',\n",
       " 'TEST_00392.jpg',\n",
       " 'TEST_00393.jpg',\n",
       " 'TEST_00394.jpg',\n",
       " 'TEST_00395.jpg',\n",
       " 'TEST_00396.jpg',\n",
       " 'TEST_00397.jpg',\n",
       " 'TEST_00398.jpg',\n",
       " 'TEST_00399.jpg',\n",
       " 'TEST_00400.jpg',\n",
       " 'TEST_00401.jpg',\n",
       " 'TEST_00402.jpg',\n",
       " 'TEST_00403.jpg',\n",
       " 'TEST_00404.jpg',\n",
       " 'TEST_00405.jpg',\n",
       " 'TEST_00406.jpg',\n",
       " 'TEST_00407.jpg',\n",
       " 'TEST_00408.jpg',\n",
       " 'TEST_00409.jpg',\n",
       " 'TEST_00410.jpg',\n",
       " 'TEST_00411.jpg',\n",
       " 'TEST_00412.jpg',\n",
       " 'TEST_00413.jpg',\n",
       " 'TEST_00414.jpg',\n",
       " 'TEST_00415.jpg',\n",
       " 'TEST_00416.jpg',\n",
       " 'TEST_00417.jpg',\n",
       " 'TEST_00418.jpg',\n",
       " 'TEST_00419.jpg',\n",
       " 'TEST_00420.jpg',\n",
       " 'TEST_00421.jpg',\n",
       " 'TEST_00422.jpg',\n",
       " 'TEST_00423.jpg',\n",
       " 'TEST_00424.jpg',\n",
       " 'TEST_00425.jpg',\n",
       " 'TEST_00426.jpg',\n",
       " 'TEST_00427.jpg',\n",
       " 'TEST_00428.jpg',\n",
       " 'TEST_00429.jpg',\n",
       " 'TEST_00430.jpg',\n",
       " 'TEST_00431.jpg',\n",
       " 'TEST_00432.jpg',\n",
       " 'TEST_00433.jpg',\n",
       " 'TEST_00434.jpg',\n",
       " 'TEST_00435.jpg',\n",
       " 'TEST_00436.jpg',\n",
       " 'TEST_00437.jpg',\n",
       " 'TEST_00438.jpg',\n",
       " 'TEST_00439.jpg',\n",
       " 'TEST_00440.jpg',\n",
       " 'TEST_00441.jpg',\n",
       " 'TEST_00442.jpg',\n",
       " 'TEST_00443.jpg',\n",
       " 'TEST_00444.jpg',\n",
       " 'TEST_00445.jpg',\n",
       " 'TEST_00446.jpg',\n",
       " 'TEST_00447.jpg',\n",
       " 'TEST_00448.jpg',\n",
       " 'TEST_00449.jpg',\n",
       " 'TEST_00450.jpg',\n",
       " 'TEST_00451.jpg',\n",
       " 'TEST_00452.jpg',\n",
       " 'TEST_00453.jpg',\n",
       " 'TEST_00454.jpg',\n",
       " 'TEST_00455.jpg',\n",
       " 'TEST_00456.jpg',\n",
       " 'TEST_00457.jpg',\n",
       " 'TEST_00458.jpg',\n",
       " 'TEST_00459.jpg',\n",
       " 'TEST_00460.jpg',\n",
       " 'TEST_00461.jpg',\n",
       " 'TEST_00462.jpg',\n",
       " 'TEST_00463.jpg',\n",
       " 'TEST_00464.jpg',\n",
       " 'TEST_00465.jpg',\n",
       " 'TEST_00466.jpg',\n",
       " 'TEST_00467.jpg',\n",
       " 'TEST_00468.jpg',\n",
       " 'TEST_00469.jpg',\n",
       " 'TEST_00470.jpg',\n",
       " 'TEST_00471.jpg',\n",
       " 'TEST_00472.jpg',\n",
       " 'TEST_00473.jpg',\n",
       " 'TEST_00474.jpg',\n",
       " 'TEST_00475.jpg',\n",
       " 'TEST_00476.jpg',\n",
       " 'TEST_00477.jpg',\n",
       " 'TEST_00478.jpg',\n",
       " 'TEST_00479.jpg',\n",
       " 'TEST_00480.jpg',\n",
       " 'TEST_00481.jpg',\n",
       " 'TEST_00482.jpg',\n",
       " 'TEST_00483.jpg',\n",
       " 'TEST_00484.jpg',\n",
       " 'TEST_00485.jpg',\n",
       " 'TEST_00486.jpg',\n",
       " 'TEST_00487.jpg',\n",
       " 'TEST_00488.jpg',\n",
       " 'TEST_00489.jpg',\n",
       " 'TEST_00490.jpg',\n",
       " 'TEST_00491.jpg',\n",
       " 'TEST_00492.jpg',\n",
       " 'TEST_00493.jpg',\n",
       " 'TEST_00494.jpg',\n",
       " 'TEST_00495.jpg',\n",
       " 'TEST_00496.jpg',\n",
       " 'TEST_00497.jpg',\n",
       " 'TEST_00498.jpg',\n",
       " 'TEST_00499.jpg',\n",
       " 'TEST_00500.jpg',\n",
       " 'TEST_00501.jpg',\n",
       " 'TEST_00502.jpg',\n",
       " 'TEST_00503.jpg',\n",
       " 'TEST_00504.jpg',\n",
       " 'TEST_00505.jpg',\n",
       " 'TEST_00506.jpg',\n",
       " 'TEST_00507.jpg',\n",
       " 'TEST_00508.jpg',\n",
       " 'TEST_00509.jpg',\n",
       " 'TEST_00510.jpg',\n",
       " 'TEST_00511.jpg',\n",
       " 'TEST_00512.jpg',\n",
       " 'TEST_00513.jpg',\n",
       " 'TEST_00514.jpg',\n",
       " 'TEST_00515.jpg',\n",
       " 'TEST_00516.jpg',\n",
       " 'TEST_00517.jpg',\n",
       " 'TEST_00518.jpg',\n",
       " 'TEST_00519.jpg',\n",
       " 'TEST_00520.jpg',\n",
       " 'TEST_00521.jpg',\n",
       " 'TEST_00522.jpg',\n",
       " 'TEST_00523.jpg',\n",
       " 'TEST_00524.jpg',\n",
       " 'TEST_00525.jpg',\n",
       " 'TEST_00526.jpg',\n",
       " 'TEST_00527.jpg',\n",
       " 'TEST_00528.jpg',\n",
       " 'TEST_00529.jpg',\n",
       " 'TEST_00530.jpg',\n",
       " 'TEST_00531.jpg',\n",
       " 'TEST_00532.jpg',\n",
       " 'TEST_00533.jpg',\n",
       " 'TEST_00534.jpg',\n",
       " 'TEST_00535.jpg',\n",
       " 'TEST_00536.jpg',\n",
       " 'TEST_00537.jpg',\n",
       " 'TEST_00538.jpg',\n",
       " 'TEST_00539.jpg',\n",
       " 'TEST_00540.jpg',\n",
       " 'TEST_00541.jpg',\n",
       " 'TEST_00542.jpg',\n",
       " 'TEST_00543.jpg',\n",
       " 'TEST_00544.jpg',\n",
       " 'TEST_00545.jpg',\n",
       " 'TEST_00546.jpg',\n",
       " 'TEST_00547.jpg',\n",
       " 'TEST_00548.jpg',\n",
       " 'TEST_00549.jpg',\n",
       " 'TEST_00550.jpg',\n",
       " 'TEST_00551.jpg',\n",
       " 'TEST_00552.jpg',\n",
       " 'TEST_00553.jpg',\n",
       " 'TEST_00554.jpg',\n",
       " 'TEST_00555.jpg',\n",
       " 'TEST_00556.jpg',\n",
       " 'TEST_00557.jpg',\n",
       " 'TEST_00558.jpg',\n",
       " 'TEST_00559.jpg',\n",
       " 'TEST_00560.jpg',\n",
       " 'TEST_00561.jpg',\n",
       " 'TEST_00562.jpg',\n",
       " 'TEST_00563.jpg',\n",
       " 'TEST_00564.jpg',\n",
       " 'TEST_00565.jpg',\n",
       " 'TEST_00566.jpg',\n",
       " 'TEST_00567.jpg',\n",
       " 'TEST_00568.jpg',\n",
       " 'TEST_00569.jpg',\n",
       " 'TEST_00570.jpg',\n",
       " 'TEST_00571.jpg',\n",
       " 'TEST_00572.jpg',\n",
       " 'TEST_00573.jpg',\n",
       " 'TEST_00574.jpg',\n",
       " 'TEST_00575.jpg',\n",
       " 'TEST_00576.jpg',\n",
       " 'TEST_00577.jpg',\n",
       " 'TEST_00578.jpg',\n",
       " 'TEST_00579.jpg',\n",
       " 'TEST_00580.jpg',\n",
       " 'TEST_00581.jpg',\n",
       " 'TEST_00582.jpg',\n",
       " 'TEST_00583.jpg',\n",
       " 'TEST_00584.jpg',\n",
       " 'TEST_00585.jpg',\n",
       " 'TEST_00586.jpg',\n",
       " 'TEST_00587.jpg',\n",
       " 'TEST_00588.jpg',\n",
       " 'TEST_00589.jpg',\n",
       " 'TEST_00590.jpg',\n",
       " 'TEST_00591.jpg',\n",
       " 'TEST_00592.jpg',\n",
       " 'TEST_00593.jpg',\n",
       " 'TEST_00594.jpg',\n",
       " 'TEST_00595.jpg',\n",
       " 'TEST_00596.jpg',\n",
       " 'TEST_00597.jpg',\n",
       " 'TEST_00598.jpg',\n",
       " 'TEST_00599.jpg',\n",
       " 'TEST_00600.jpg',\n",
       " 'TEST_00601.jpg',\n",
       " 'TEST_00602.jpg',\n",
       " 'TEST_00603.jpg',\n",
       " 'TEST_00604.jpg',\n",
       " 'TEST_00605.jpg',\n",
       " 'TEST_00606.jpg',\n",
       " 'TEST_00607.jpg',\n",
       " 'TEST_00608.jpg',\n",
       " 'TEST_00609.jpg',\n",
       " 'TEST_00610.jpg',\n",
       " 'TEST_00611.jpg',\n",
       " 'TEST_00612.jpg',\n",
       " 'TEST_00613.jpg',\n",
       " 'TEST_00614.jpg',\n",
       " 'TEST_00615.jpg',\n",
       " 'TEST_00616.jpg',\n",
       " 'TEST_00617.jpg',\n",
       " 'TEST_00618.jpg',\n",
       " 'TEST_00619.jpg',\n",
       " 'TEST_00620.jpg',\n",
       " 'TEST_00621.jpg',\n",
       " 'TEST_00622.jpg',\n",
       " 'TEST_00623.jpg',\n",
       " 'TEST_00624.jpg',\n",
       " 'TEST_00625.jpg',\n",
       " 'TEST_00626.jpg',\n",
       " 'TEST_00627.jpg',\n",
       " 'TEST_00628.jpg',\n",
       " 'TEST_00629.jpg',\n",
       " 'TEST_00630.jpg',\n",
       " 'TEST_00631.jpg',\n",
       " 'TEST_00632.jpg',\n",
       " 'TEST_00633.jpg',\n",
       " 'TEST_00634.jpg',\n",
       " 'TEST_00635.jpg',\n",
       " 'TEST_00636.jpg',\n",
       " 'TEST_00637.jpg',\n",
       " 'TEST_00638.jpg',\n",
       " 'TEST_00639.jpg',\n",
       " 'TEST_00640.jpg',\n",
       " 'TEST_00641.jpg',\n",
       " 'TEST_00642.jpg',\n",
       " 'TEST_00643.jpg',\n",
       " 'TEST_00644.jpg',\n",
       " 'TEST_00645.jpg',\n",
       " 'TEST_00646.jpg',\n",
       " 'TEST_00647.jpg',\n",
       " 'TEST_00648.jpg',\n",
       " 'TEST_00649.jpg',\n",
       " 'TEST_00650.jpg',\n",
       " 'TEST_00651.jpg',\n",
       " 'TEST_00652.jpg',\n",
       " 'TEST_00653.jpg',\n",
       " 'TEST_00654.jpg',\n",
       " 'TEST_00655.jpg',\n",
       " 'TEST_00656.jpg',\n",
       " 'TEST_00657.jpg',\n",
       " 'TEST_00658.jpg',\n",
       " 'TEST_00659.jpg',\n",
       " 'TEST_00660.jpg',\n",
       " 'TEST_00661.jpg',\n",
       " 'TEST_00662.jpg',\n",
       " 'TEST_00663.jpg',\n",
       " 'TEST_00664.jpg',\n",
       " 'TEST_00665.jpg',\n",
       " 'TEST_00666.jpg',\n",
       " 'TEST_00667.jpg',\n",
       " 'TEST_00668.jpg',\n",
       " 'TEST_00669.jpg',\n",
       " 'TEST_00670.jpg',\n",
       " 'TEST_00671.jpg',\n",
       " 'TEST_00672.jpg',\n",
       " 'TEST_00673.jpg',\n",
       " 'TEST_00674.jpg',\n",
       " 'TEST_00675.jpg',\n",
       " 'TEST_00676.jpg',\n",
       " 'TEST_00677.jpg',\n",
       " 'TEST_00678.jpg',\n",
       " 'TEST_00679.jpg',\n",
       " 'TEST_00680.jpg',\n",
       " 'TEST_00681.jpg',\n",
       " 'TEST_00682.jpg',\n",
       " 'TEST_00683.jpg',\n",
       " 'TEST_00684.jpg',\n",
       " 'TEST_00685.jpg',\n",
       " 'TEST_00686.jpg',\n",
       " 'TEST_00687.jpg',\n",
       " 'TEST_00688.jpg',\n",
       " 'TEST_00689.jpg',\n",
       " 'TEST_00690.jpg',\n",
       " 'TEST_00691.jpg',\n",
       " 'TEST_00692.jpg',\n",
       " 'TEST_00693.jpg',\n",
       " 'TEST_00694.jpg',\n",
       " 'TEST_00695.jpg',\n",
       " 'TEST_00696.jpg',\n",
       " 'TEST_00697.jpg',\n",
       " 'TEST_00698.jpg',\n",
       " 'TEST_00699.jpg',\n",
       " 'TEST_00700.jpg',\n",
       " 'TEST_00701.jpg',\n",
       " 'TEST_00702.jpg',\n",
       " 'TEST_00703.jpg',\n",
       " 'TEST_00704.jpg',\n",
       " 'TEST_00705.jpg',\n",
       " 'TEST_00706.jpg',\n",
       " 'TEST_00707.jpg',\n",
       " 'TEST_00708.jpg',\n",
       " 'TEST_00709.jpg',\n",
       " 'TEST_00710.jpg',\n",
       " 'TEST_00711.jpg',\n",
       " 'TEST_00712.jpg',\n",
       " 'TEST_00713.jpg',\n",
       " 'TEST_00714.jpg',\n",
       " 'TEST_00715.jpg',\n",
       " 'TEST_00716.jpg',\n",
       " 'TEST_00717.jpg',\n",
       " 'TEST_00718.jpg',\n",
       " 'TEST_00719.jpg',\n",
       " 'TEST_00720.jpg',\n",
       " 'TEST_00721.jpg',\n",
       " 'TEST_00722.jpg',\n",
       " 'TEST_00723.jpg',\n",
       " 'TEST_00724.jpg',\n",
       " 'TEST_00725.jpg',\n",
       " 'TEST_00726.jpg',\n",
       " 'TEST_00727.jpg',\n",
       " 'TEST_00728.jpg',\n",
       " 'TEST_00729.jpg',\n",
       " 'TEST_00730.jpg',\n",
       " 'TEST_00731.jpg',\n",
       " 'TEST_00732.jpg',\n",
       " 'TEST_00733.jpg',\n",
       " 'TEST_00734.jpg',\n",
       " 'TEST_00735.jpg',\n",
       " 'TEST_00736.jpg',\n",
       " 'TEST_00737.jpg',\n",
       " 'TEST_00738.jpg',\n",
       " 'TEST_00739.jpg',\n",
       " 'TEST_00740.jpg',\n",
       " 'TEST_00741.jpg',\n",
       " 'TEST_00742.jpg',\n",
       " 'TEST_00743.jpg',\n",
       " 'TEST_00744.jpg',\n",
       " 'TEST_00745.jpg',\n",
       " 'TEST_00746.jpg',\n",
       " 'TEST_00747.jpg',\n",
       " 'TEST_00748.jpg',\n",
       " 'TEST_00749.jpg',\n",
       " 'TEST_00750.jpg',\n",
       " 'TEST_00751.jpg',\n",
       " 'TEST_00752.jpg',\n",
       " 'TEST_00753.jpg',\n",
       " 'TEST_00754.jpg',\n",
       " 'TEST_00755.jpg',\n",
       " 'TEST_00756.jpg',\n",
       " 'TEST_00757.jpg',\n",
       " 'TEST_00758.jpg',\n",
       " 'TEST_00759.jpg',\n",
       " 'TEST_00760.jpg',\n",
       " 'TEST_00761.jpg',\n",
       " 'TEST_00762.jpg',\n",
       " 'TEST_00763.jpg',\n",
       " 'TEST_00764.jpg',\n",
       " 'TEST_00765.jpg',\n",
       " 'TEST_00766.jpg',\n",
       " 'TEST_00767.jpg',\n",
       " 'TEST_00768.jpg',\n",
       " 'TEST_00769.jpg',\n",
       " 'TEST_00770.jpg',\n",
       " 'TEST_00771.jpg',\n",
       " 'TEST_00772.jpg',\n",
       " 'TEST_00773.jpg',\n",
       " 'TEST_00774.jpg',\n",
       " 'TEST_00775.jpg',\n",
       " 'TEST_00776.jpg',\n",
       " 'TEST_00777.jpg',\n",
       " 'TEST_00778.jpg',\n",
       " 'TEST_00779.jpg',\n",
       " 'TEST_00780.jpg',\n",
       " 'TEST_00781.jpg',\n",
       " 'TEST_00782.jpg',\n",
       " 'TEST_00783.jpg',\n",
       " 'TEST_00784.jpg',\n",
       " 'TEST_00785.jpg',\n",
       " 'TEST_00786.jpg',\n",
       " 'TEST_00787.jpg',\n",
       " 'TEST_00788.jpg',\n",
       " 'TEST_00789.jpg',\n",
       " 'TEST_00790.jpg',\n",
       " 'TEST_00791.jpg',\n",
       " 'TEST_00792.jpg',\n",
       " 'TEST_00793.jpg',\n",
       " 'TEST_00794.jpg',\n",
       " 'TEST_00795.jpg',\n",
       " 'TEST_00796.jpg',\n",
       " 'TEST_00797.jpg',\n",
       " 'TEST_00798.jpg',\n",
       " 'TEST_00799.jpg',\n",
       " 'TEST_00800.jpg',\n",
       " 'TEST_00801.jpg',\n",
       " 'TEST_00802.jpg',\n",
       " 'TEST_00803.jpg',\n",
       " 'TEST_00804.jpg',\n",
       " 'TEST_00805.jpg',\n",
       " 'TEST_00806.jpg',\n",
       " 'TEST_00807.jpg',\n",
       " 'TEST_00808.jpg',\n",
       " 'TEST_00809.jpg',\n",
       " 'TEST_00810.jpg',\n",
       " 'TEST_00811.jpg',\n",
       " 'TEST_00812.jpg',\n",
       " 'TEST_00813.jpg',\n",
       " 'TEST_00814.jpg',\n",
       " 'TEST_00815.jpg',\n",
       " 'TEST_00816.jpg',\n",
       " 'TEST_00817.jpg',\n",
       " 'TEST_00818.jpg',\n",
       " 'TEST_00819.jpg',\n",
       " 'TEST_00820.jpg',\n",
       " 'TEST_00821.jpg',\n",
       " 'TEST_00822.jpg',\n",
       " 'TEST_00823.jpg',\n",
       " 'TEST_00824.jpg',\n",
       " 'TEST_00825.jpg',\n",
       " 'TEST_00826.jpg',\n",
       " 'TEST_00827.jpg',\n",
       " 'TEST_00828.jpg',\n",
       " 'TEST_00829.jpg',\n",
       " 'TEST_00830.jpg',\n",
       " 'TEST_00831.jpg',\n",
       " 'TEST_00832.jpg',\n",
       " 'TEST_00833.jpg',\n",
       " 'TEST_00834.jpg',\n",
       " 'TEST_00835.jpg',\n",
       " 'TEST_00836.jpg',\n",
       " 'TEST_00837.jpg',\n",
       " 'TEST_00838.jpg',\n",
       " 'TEST_00839.jpg',\n",
       " 'TEST_00840.jpg',\n",
       " 'TEST_00841.jpg',\n",
       " 'TEST_00842.jpg',\n",
       " 'TEST_00843.jpg',\n",
       " 'TEST_00844.jpg',\n",
       " 'TEST_00845.jpg',\n",
       " 'TEST_00846.jpg',\n",
       " 'TEST_00847.jpg',\n",
       " 'TEST_00848.jpg',\n",
       " 'TEST_00849.jpg',\n",
       " 'TEST_00850.jpg',\n",
       " 'TEST_00851.jpg',\n",
       " 'TEST_00852.jpg',\n",
       " 'TEST_00853.jpg',\n",
       " 'TEST_00854.jpg',\n",
       " 'TEST_00855.jpg',\n",
       " 'TEST_00856.jpg',\n",
       " 'TEST_00857.jpg',\n",
       " 'TEST_00858.jpg',\n",
       " 'TEST_00859.jpg',\n",
       " 'TEST_00860.jpg',\n",
       " 'TEST_00861.jpg',\n",
       " 'TEST_00862.jpg',\n",
       " 'TEST_00863.jpg',\n",
       " 'TEST_00864.jpg',\n",
       " 'TEST_00865.jpg',\n",
       " 'TEST_00866.jpg',\n",
       " 'TEST_00867.jpg',\n",
       " 'TEST_00868.jpg',\n",
       " 'TEST_00869.jpg',\n",
       " 'TEST_00870.jpg',\n",
       " 'TEST_00871.jpg',\n",
       " 'TEST_00872.jpg',\n",
       " 'TEST_00873.jpg',\n",
       " 'TEST_00874.jpg',\n",
       " 'TEST_00875.jpg',\n",
       " 'TEST_00876.jpg',\n",
       " 'TEST_00877.jpg',\n",
       " 'TEST_00878.jpg',\n",
       " 'TEST_00879.jpg',\n",
       " 'TEST_00880.jpg',\n",
       " 'TEST_00881.jpg',\n",
       " 'TEST_00882.jpg',\n",
       " 'TEST_00883.jpg',\n",
       " 'TEST_00884.jpg',\n",
       " 'TEST_00885.jpg',\n",
       " 'TEST_00886.jpg',\n",
       " 'TEST_00887.jpg',\n",
       " 'TEST_00888.jpg',\n",
       " 'TEST_00889.jpg',\n",
       " 'TEST_00890.jpg',\n",
       " 'TEST_00891.jpg',\n",
       " 'TEST_00892.jpg',\n",
       " 'TEST_00893.jpg',\n",
       " 'TEST_00894.jpg',\n",
       " 'TEST_00895.jpg',\n",
       " 'TEST_00896.jpg',\n",
       " 'TEST_00897.jpg',\n",
       " 'TEST_00898.jpg',\n",
       " 'TEST_00899.jpg',\n",
       " 'TEST_00900.jpg',\n",
       " 'TEST_00901.jpg',\n",
       " 'TEST_00902.jpg',\n",
       " 'TEST_00903.jpg',\n",
       " 'TEST_00904.jpg',\n",
       " 'TEST_00905.jpg',\n",
       " 'TEST_00906.jpg',\n",
       " 'TEST_00907.jpg',\n",
       " 'TEST_00908.jpg',\n",
       " 'TEST_00909.jpg',\n",
       " 'TEST_00910.jpg',\n",
       " 'TEST_00911.jpg',\n",
       " 'TEST_00912.jpg',\n",
       " 'TEST_00913.jpg',\n",
       " 'TEST_00914.jpg',\n",
       " 'TEST_00915.jpg',\n",
       " 'TEST_00916.jpg',\n",
       " 'TEST_00917.jpg',\n",
       " 'TEST_00918.jpg',\n",
       " 'TEST_00919.jpg',\n",
       " 'TEST_00920.jpg',\n",
       " 'TEST_00921.jpg',\n",
       " 'TEST_00922.jpg',\n",
       " 'TEST_00923.jpg',\n",
       " 'TEST_00924.jpg',\n",
       " 'TEST_00925.jpg',\n",
       " 'TEST_00926.jpg',\n",
       " 'TEST_00927.jpg',\n",
       " 'TEST_00928.jpg',\n",
       " 'TEST_00929.jpg',\n",
       " 'TEST_00930.jpg',\n",
       " 'TEST_00931.jpg',\n",
       " 'TEST_00932.jpg',\n",
       " 'TEST_00933.jpg',\n",
       " 'TEST_00934.jpg',\n",
       " 'TEST_00935.jpg',\n",
       " 'TEST_00936.jpg',\n",
       " 'TEST_00937.jpg',\n",
       " 'TEST_00938.jpg',\n",
       " 'TEST_00939.jpg',\n",
       " 'TEST_00940.jpg',\n",
       " 'TEST_00941.jpg',\n",
       " 'TEST_00942.jpg',\n",
       " 'TEST_00943.jpg',\n",
       " 'TEST_00944.jpg',\n",
       " 'TEST_00945.jpg',\n",
       " 'TEST_00946.jpg',\n",
       " 'TEST_00947.jpg',\n",
       " 'TEST_00948.jpg',\n",
       " 'TEST_00949.jpg',\n",
       " 'TEST_00950.jpg',\n",
       " 'TEST_00951.jpg',\n",
       " 'TEST_00952.jpg',\n",
       " 'TEST_00953.jpg',\n",
       " 'TEST_00954.jpg',\n",
       " 'TEST_00955.jpg',\n",
       " 'TEST_00956.jpg',\n",
       " 'TEST_00957.jpg',\n",
       " 'TEST_00958.jpg',\n",
       " 'TEST_00959.jpg',\n",
       " 'TEST_00960.jpg',\n",
       " 'TEST_00961.jpg',\n",
       " 'TEST_00962.jpg',\n",
       " 'TEST_00963.jpg',\n",
       " 'TEST_00964.jpg',\n",
       " 'TEST_00965.jpg',\n",
       " 'TEST_00966.jpg',\n",
       " 'TEST_00967.jpg',\n",
       " 'TEST_00968.jpg',\n",
       " 'TEST_00969.jpg',\n",
       " 'TEST_00970.jpg',\n",
       " 'TEST_00971.jpg',\n",
       " 'TEST_00972.jpg',\n",
       " 'TEST_00973.jpg',\n",
       " 'TEST_00974.jpg',\n",
       " 'TEST_00975.jpg',\n",
       " 'TEST_00976.jpg',\n",
       " 'TEST_00977.jpg',\n",
       " 'TEST_00978.jpg',\n",
       " 'TEST_00979.jpg',\n",
       " 'TEST_00980.jpg',\n",
       " 'TEST_00981.jpg',\n",
       " 'TEST_00982.jpg',\n",
       " 'TEST_00983.jpg',\n",
       " 'TEST_00984.jpg',\n",
       " 'TEST_00985.jpg',\n",
       " 'TEST_00986.jpg',\n",
       " 'TEST_00987.jpg',\n",
       " 'TEST_00988.jpg',\n",
       " 'TEST_00989.jpg',\n",
       " 'TEST_00990.jpg',\n",
       " 'TEST_00991.jpg',\n",
       " 'TEST_00992.jpg',\n",
       " 'TEST_00993.jpg',\n",
       " 'TEST_00994.jpg',\n",
       " 'TEST_00995.jpg',\n",
       " 'TEST_00996.jpg',\n",
       " 'TEST_00997.jpg',\n",
       " 'TEST_00998.jpg',\n",
       " 'TEST_00999.jpg',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list = [f\"{k}/{i}\" for k in train_class for i in os.listdir(f\"{train_path}/{k}\")]\n",
    "test_list = sorted([i for i in os.listdir(test_path)])\n",
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CutoutCustom(A.ImageOnlyTransform):\n",
    "    def __init__(self, size=50, always_apply=False, p=0.5):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.size = size\n",
    "\n",
    "    def apply(self, image, **params):\n",
    "        H, W, _ = image.shape\n",
    "        x = np.random.randint(W)\n",
    "        y = np.random.randint(H)\n",
    "        x1 = np.clip(x - self.size // 2, 0, W)\n",
    "        x2 = np.clip(x + self.size // 2, 0, W)\n",
    "        y1 = np.clip(y - self.size // 2, 0, H)\n",
    "        y2 = np.clip(y + self.size // 2, 0, H)\n",
    "        image[y1:y2, x1:x2, :] = 0\n",
    "        return image\n",
    "\n",
    "\n",
    "class BlockMasking(A.ImageOnlyTransform):\n",
    "    def __init__(self, num_blocks=20, block_size=20, always_apply=False, p=0.5):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.num_blocks = num_blocks\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def apply(self, image, **params):\n",
    "        H, W, _ = image.shape\n",
    "        for _ in range(self.num_blocks):\n",
    "            x = np.random.randint(0, W - self.block_size)\n",
    "            y = np.random.randint(0, H - self.block_size)\n",
    "            image[y:y + self.block_size, x:x + self.block_size, :] = 0\n",
    "        return image\n",
    "    \n",
    "class HalfCrop(A.ImageOnlyTransform):\n",
    "    def __init__(self, mode='random', always_apply=False, p=0.5):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.mode = mode\n",
    "\n",
    "    def apply(self, image, **params):\n",
    "        H, W, C = image.shape\n",
    "\n",
    "        if self.mode == 'random':\n",
    "            mode = np.random.choice(['left', 'right', 'top', 'bottom'])\n",
    "        else:\n",
    "            mode = self.mode\n",
    "\n",
    "        if mode == 'left':\n",
    "            return image[:, :W // 2, :]\n",
    "        elif mode == 'right':\n",
    "            return image[:, W // 2:, :]\n",
    "        elif mode == 'top':\n",
    "            return image[:H // 2, :, :]\n",
    "        elif mode == 'bottom':\n",
    "            return image[H // 2:, :, :]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode: {mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, base_path, path_list, mode='train', transform=None, label_encoder=None):\n",
    "        self.path = base_path\n",
    "        self.data = path_list\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        img_path = self.data[item]\n",
    "        image = cv2.imread(f\"{self.path}/{img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            label_str = img_path.split(\"/\")[0]\n",
    "            label = self.label_encoder.transform([label_str])[0]\n",
    "            return image, label\n",
    "        elif self.mode == \"test\":\n",
    "            return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CamAttentionCrop(A.ImageOnlyTransform):\n",
    "    def __init__(self, model, target_layer, size=(384, 384), device='cuda', p=1.0):\n",
    "        super().__init__(always_apply=False, p=p)\n",
    "        self.model = model.eval()\n",
    "        self.device = device\n",
    "        self.size = size\n",
    "        self.target_layer = target_layer\n",
    "        self.activation = None\n",
    "\n",
    "        # Register forward hook\n",
    "        self.target_layer.register_forward_hook(self._hook)\n",
    "\n",
    "        # Fallback transform\n",
    "        self.fallback_transform = A.RandomResizedCrop(\n",
    "            size=size,  # ✅ 올바른 방식\n",
    "            scale=(0.5, 1.0), ratio=(0.75, 1.33),\n",
    "            p=1.0\n",
    "        )\n",
    "\n",
    "    def _hook(self, module, input, output):\n",
    "        # Save activation from LayerNorm (B, C)\n",
    "        self.activation = output.detach()\n",
    "\n",
    "    def apply(self, image, **params):\n",
    "        try:\n",
    "            # Convert image to tensor and normalize [0, 1]\n",
    "            image_tensor = torch.from_numpy(image).permute(2, 0, 1).float().unsqueeze(0) / 255.\n",
    "            image_tensor = image_tensor.to(self.device)\n",
    "\n",
    "            _ = self.model(image_tensor)\n",
    "            cam = self.activation.squeeze(0)  # shape: (C,)\n",
    "            cam_np = cam.cpu().numpy()\n",
    "\n",
    "            # Map feature vector back to 2D spatial layout\n",
    "            H, W, _ = image.shape\n",
    "            side = int(np.sqrt(len(cam_np)))\n",
    "            if side * side != len(cam_np):\n",
    "                raise ValueError(\"Cannot reshape activation to square\")\n",
    "\n",
    "            cam_2d = cam_np.reshape(side, side)\n",
    "            cam_resized = cv2.resize(cam_2d, (W, H), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            # Max point (center of attention)\n",
    "            y, x = np.unravel_index(np.argmax(cam_resized), cam_resized.shape)\n",
    "            crop_h, crop_w = self.size\n",
    "\n",
    "            x1 = max(x - crop_w // 2, 0)\n",
    "            y1 = max(y - crop_h // 2, 0)\n",
    "            x2 = min(x1 + crop_w, W)\n",
    "            y2 = min(y1 + crop_h, H)\n",
    "\n",
    "            if (x2 - x1 < crop_w) or (y2 - y1 < crop_h):\n",
    "                raise ValueError(\"CAM crop too small, fallback to RandomResizedCrop\")\n",
    "\n",
    "            cropped = image[y1:y2, x1:x2]\n",
    "            cropped = cv2.resize(cropped, self.size, interpolation=cv2.INTER_LINEAR)\n",
    "            return cropped\n",
    "\n",
    "        except Exception as e:\n",
    "            # Fallback cropping\n",
    "            fallback = self.fallback_transform(image=image)\n",
    "            return fallback[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1시리즈_F20_2013_2015',\n",
       " '1시리즈_F20_2016_2019',\n",
       " '1시리즈_F40_2020_2024',\n",
       " '2008_2015_2017',\n",
       " '2시리즈_그란쿠페_F44_2020_2024',\n",
       " '2시리즈_액티브_투어러_F45_2019_2021',\n",
       " '2시리즈_액티브_투어러_U06_2022_2024',\n",
       " '3008_2세대_2018_2023',\n",
       " '3시리즈_E90_2005_2012',\n",
       " '3시리즈_F30_2013_2018',\n",
       " '3시리즈_G20_2019_2022',\n",
       " '3시리즈_G20_2023_2025',\n",
       " '3시리즈_GT_F34_2014_2021',\n",
       " '4시리즈_F32_2014_2020',\n",
       " '4시리즈_G22_2021_2023',\n",
       " '4시리즈_G22_2024_2025',\n",
       " '5008_2세대_2018_2019',\n",
       " '5008_2세대_2021_2024',\n",
       " '5시리즈_F10_2010_2016',\n",
       " '5시리즈_G30_2017_2023',\n",
       " '5시리즈_G60_2024_2025',\n",
       " '5시리즈_GT_F07_2010_2017',\n",
       " '6시리즈_F12_2011_2018',\n",
       " '6시리즈_GT_G32_2018_2020',\n",
       " '6시리즈_GT_G32_2021_2024',\n",
       " '718_박스터_2017_2024',\n",
       " '718_카이맨_2017_2024',\n",
       " '7시리즈_F01_2009_2015',\n",
       " '7시리즈_G11_2016_2018',\n",
       " '7시리즈_G11_2019_2022',\n",
       " '7시리즈_G70_2023_2025',\n",
       " '8시리즈_G15_2020_2024',\n",
       " '911_2003_2019',\n",
       " '911_992_2020_2024',\n",
       " 'A4_B9_2016_2019',\n",
       " 'A4_B9_2020_2024',\n",
       " 'A5_F5_2019_2024',\n",
       " 'A6_C8_2019_2025',\n",
       " 'A7_2012_2016',\n",
       " 'A7_4K_2020_2024',\n",
       " 'A8_D5_2018_2023',\n",
       " 'AMG_GT_2016_2024',\n",
       " 'A_클래스_W176_2015_2018',\n",
       " 'A_클래스_W177_2020_2025',\n",
       " 'All_New_XJ_2016_2019',\n",
       " 'B_클래스_W246_2013_2018',\n",
       " 'CLA_클래스_C117_2014_2019',\n",
       " 'CLA_클래스_C118_2020_2025',\n",
       " 'CLE_클래스_C236_2024_2025',\n",
       " 'CLS_클래스_C257_2019_2023',\n",
       " 'CLS_클래스_W218_2012_2017',\n",
       " 'CT6_2016_2018',\n",
       " 'C_클래스_W204_2008_2015',\n",
       " 'C_클래스_W205_2015_2021',\n",
       " 'C_클래스_W206_2022_2024',\n",
       " 'EQ900_2016_2018',\n",
       " 'EQA_H243_2021_2024',\n",
       " 'EQE_V295_2022_2024',\n",
       " 'EQS_V297_2022_2023',\n",
       " 'ES300h_7세대_2019_2026',\n",
       " 'EV6_2022_2024',\n",
       " 'EV9_2024_2025',\n",
       " 'E_PACE_2018_2020',\n",
       " 'E_클래스_W212_2010_2016',\n",
       " 'E_클래스_W213_2017_2020',\n",
       " 'E_클래스_W213_2021_2023',\n",
       " 'E_클래스_W214_2024_2025',\n",
       " 'F150_2004_2021',\n",
       " 'F_PACE_2017_2019',\n",
       " 'G4_렉스턴_2018_2020',\n",
       " 'G70_2018_2020',\n",
       " 'G80_2017_2020',\n",
       " 'G80_RG3_2021_2023',\n",
       " 'G80_RG3_2025',\n",
       " 'G90_2019_2022',\n",
       " 'G90_RS4_2022_2025',\n",
       " 'GLA_클래스_H247_2020_2025',\n",
       " 'GLA_클래스_X156_2015_2019',\n",
       " 'GLB_클래스_X247_2020_2023',\n",
       " 'GLC_클래스_X253_2017_2019',\n",
       " 'GLC_클래스_X253_2020_2022',\n",
       " 'GLC_클래스_X253_2023',\n",
       " 'GLC_클래스_X254_2023_2025',\n",
       " 'GLE_클래스_W166_2016_2018',\n",
       " 'GLE_클래스_W167_2019_2024',\n",
       " 'GLS_클래스_X166_2017_2019',\n",
       " 'GLS_클래스_X167_2020_2024',\n",
       " 'GV70_2021_2023',\n",
       " 'GV80_2020_2022',\n",
       " 'GV80_2024_2025',\n",
       " 'G_클래스_W463_2009_2017',\n",
       " 'G_클래스_W463b_2019_2025',\n",
       " 'K3_2013_2015',\n",
       " 'K5_2세대_2016_2018',\n",
       " 'K5_3세대_2020_2023',\n",
       " 'K5_3세대_하이브리드_2020_2022',\n",
       " 'K5_하이브리드_3세대_2020_2023',\n",
       " 'K7_프리미어_2020_2021',\n",
       " 'K7_프리미어_하이브리드_2020_2021',\n",
       " 'K8_2022_2024',\n",
       " 'K8_하이브리드_2022_2024',\n",
       " 'LF_쏘나타_2015_2017',\n",
       " 'M2_F87_2016_2021',\n",
       " 'M4_F82_2015_2020',\n",
       " 'M5_F90_2018_2023',\n",
       " 'MKC_2015_2018',\n",
       " 'New_XF_2012_2015',\n",
       " 'Q30_2017_2019',\n",
       " 'Q3_F3_2020_2024',\n",
       " 'Q50_2014_2017',\n",
       " 'Q5_FY_2020',\n",
       " 'Q5_FY_2021_2024',\n",
       " 'Q7_4M_2016_2019',\n",
       " 'Q7_4M_2020_2023',\n",
       " 'Q8_4M_2020_2025',\n",
       " 'QM3_2014_2017',\n",
       " 'QM6_2017_2019',\n",
       " 'QX60_2016_2018',\n",
       " 'RAV4_2016_2018',\n",
       " 'RAV4_5세대_2019_2024',\n",
       " 'S60_3세대_2020_2024',\n",
       " 'S90_2017_2020',\n",
       " 'S90_2021_2025',\n",
       " 'SM3_네오_2015_2019',\n",
       " 'SM5_노바_2015_2019',\n",
       " 'SM6_2016_2020',\n",
       " 'SM7_노바_2015_2019',\n",
       " 'SM7_뉴아트_2008_2011',\n",
       " 'S_클래스_W221_2006_2013',\n",
       " 'S_클래스_W222_2014_2020',\n",
       " 'S_클래스_W223_2021_2025',\n",
       " 'UX250h_2019_2024',\n",
       " 'V40_2015_2018',\n",
       " 'V60_크로스컨트리_2세대_2020_2025',\n",
       " 'V90_크로스컨트리_2018_2024',\n",
       " 'X1_F48_2016_2019',\n",
       " 'X1_F48_2020_2022',\n",
       " 'X1_U11_2023_2024',\n",
       " 'X2_F39_2018_2023',\n",
       " 'X3_G01_2018_2021',\n",
       " 'X3_G01_2022_2024',\n",
       " 'X4_F26_2015_2018',\n",
       " 'X4_G02_2019_2021',\n",
       " 'X4_G02_2022_2025',\n",
       " 'X5_F15_2014_2018',\n",
       " 'X5_G05_2019_2023',\n",
       " 'X5_G05_2024_2025',\n",
       " 'X6_F16_2015_2019',\n",
       " 'X6_G06_2020_2023',\n",
       " 'X6_G06_2024_2025',\n",
       " 'X7_G07_2019_2022',\n",
       " 'X7_G07_2023_2025',\n",
       " 'XC40_2019_2022',\n",
       " 'XC60_2세대_2018_2021',\n",
       " 'XC60_2세대_2022_2025',\n",
       " 'XC90_2세대_2017_2019',\n",
       " 'XC90_2세대_2020_2025',\n",
       " 'XE_2016_2019',\n",
       " 'XF_X260_2016_2020',\n",
       " 'XJ_8세대_2010_2019',\n",
       " 'XM3_2020_2023',\n",
       " 'XM3_2024',\n",
       " 'YF쏘나타_2009_2012',\n",
       " 'YF쏘나타_하이브리드_2011_2015',\n",
       " 'Z4_G29_2019_2025',\n",
       " 'e_트론_2020_2023',\n",
       " 'i30_PD_2017_2018',\n",
       " 'i4_2022_2024',\n",
       " 'iX_2022_2024',\n",
       " '골프_7세대_2013_2016',\n",
       " '그랑_콜레오스_2025',\n",
       " '그랜드_스타렉스_2016_2018',\n",
       " '그랜드_체로키_2014_2020',\n",
       " '그랜드_체로키_WL_2021_2023',\n",
       " '그랜드카니발_2006_2010',\n",
       " '그랜저TG_2007_2008',\n",
       " '그랜저_GN7_2023_2025',\n",
       " '그랜저_HG_2011_2014',\n",
       " '그랜저_HG_2015_2017',\n",
       " '그랜저_IG_2017_2019',\n",
       " '글래디에이터_JT_2020_2023',\n",
       " '기블리_2014_2023',\n",
       " '넥쏘_2018_2024',\n",
       " '뉴QM3_2018_2019',\n",
       " '뉴_A6_2012_2014',\n",
       " '뉴_A6_2015_2018',\n",
       " '뉴_CC_2012_2016',\n",
       " '뉴_ES300h_2013_2015',\n",
       " '뉴_ES300h_2016_2018',\n",
       " '뉴_G80_2025_2026',\n",
       " '뉴_GV80_2024_2025',\n",
       " '뉴_MKZ_2017_2020',\n",
       " '뉴_QM5_2012_2014',\n",
       " '뉴_QM6_2021_2023',\n",
       " '뉴_SM5_임프레션_2008_2010',\n",
       " '뉴_SM5_플래티넘_2013_2014',\n",
       " '뉴_스타일_코란도_C_2017_2019',\n",
       " '뉴_제타_2011_2016',\n",
       " '뉴_체어맨_W_2012_2016',\n",
       " '뉴_카이엔_2011_2018',\n",
       " '뉴_티구안_2012_2016',\n",
       " '뉴쏘렌토_R_2013_2014',\n",
       " '니로_2017_2019',\n",
       " '더_K9_2019_2021',\n",
       " '더_기아_레이_EV_2024_2025',\n",
       " '더_넥스트_스파크_2016_2018',\n",
       " '더_뉴_G70_2021_2025',\n",
       " '더_뉴_K3_2016_2018',\n",
       " '더_뉴_K3_2세대_2022_2024',\n",
       " '더_뉴_K5_2세대_2019_2020',\n",
       " '더_뉴_K5_3세대_2024_2025',\n",
       " '더_뉴_K5_하이브리드_3세대_2023_2025',\n",
       " '더_뉴_K7_2013_2016',\n",
       " '더_뉴_K9_2세대_2022_2025',\n",
       " '더_뉴_QM6_2020_2023',\n",
       " '더_뉴_QM6_2024_2025',\n",
       " '더_뉴_SM6_2021_2024',\n",
       " '더_뉴_그랜드_스타렉스_2018_2021',\n",
       " '더_뉴_그랜저_IG_2020_2023',\n",
       " '더_뉴_기아_레이_2022_2025',\n",
       " '더_뉴_니로_2020_2022',\n",
       " '더_뉴_레이_2018_2022',\n",
       " '더_뉴_렉스턴_스포츠_2021_2025',\n",
       " '더_뉴_렉스턴_스포츠_칸_2021_2025',\n",
       " '더_뉴_말리부_2019_2022',\n",
       " '더_뉴_맥스크루즈_2016_2018',\n",
       " '더_뉴_모닝_2015_2016',\n",
       " '더_뉴_모닝_JA_2024_2025',\n",
       " '더_뉴_모하비_2017_2019',\n",
       " '더_뉴_셀토스_2023_2025',\n",
       " '더_뉴_스파크_2019_2022',\n",
       " '더_뉴_싼타페_2021_2023',\n",
       " '더_뉴_쏘렌토_2018_2020',\n",
       " '더_뉴_쏘렌토_4세대_2024_2025',\n",
       " '더_뉴_아반떼_2014_2016',\n",
       " '더_뉴_아반떼_AD_2019_2020',\n",
       " '더_뉴_아반떼_CN7_2023_2025',\n",
       " '더_뉴_아이오닉_하이브리드_2020',\n",
       " '더_뉴_카니발_2019_2020',\n",
       " '더_뉴_카니발_4세대_2024_2025',\n",
       " '더_뉴_코나_2021_2023',\n",
       " '더_뉴_코란도_스포츠_2016_2018',\n",
       " '더_뉴_투싼_NX4_2023_2025',\n",
       " '더_뉴_트랙스_2017_2022',\n",
       " '더_뉴_파사트_2012_2019',\n",
       " '더_뉴_팰리세이드_2023_2024',\n",
       " '더_뉴스포티지R_2014_2016',\n",
       " '더_올뉴G80_2021_2024',\n",
       " '더_올뉴투싼_하이브리드_2021_2023',\n",
       " '디_올_뉴_니로_2022_2025',\n",
       " '디_올_뉴_스포티지_2022_2024',\n",
       " '디_올뉴그랜저_2023_2025',\n",
       " '디_올뉴니로EV_2023_2024',\n",
       " '디_올뉴니로_2022_2025',\n",
       " '디_올뉴싼타페_2024_2025',\n",
       " '디_올뉴코나_2023_2025',\n",
       " '디스커버리_4_2010_2016',\n",
       " '디스커버리_5_2017_2020',\n",
       " '디스커버리_5_2022_2024',\n",
       " '디스커버리_스포츠_2015_2019',\n",
       " '디스커버리_스포츠_2세대_2020_2025',\n",
       " '디펜더_L663_2020_2025',\n",
       " '라브4_4세대_2013_2018',\n",
       " '라브4_5세대_2019_2024',\n",
       " '랭글러_JK_2009_2017',\n",
       " '랭글러_JL_2018_2024',\n",
       " '레니게이드_2015_2017',\n",
       " '레니게이드_2019_2023',\n",
       " '레이_2012_2017',\n",
       " '레인지로버_4세대_2014_2017',\n",
       " '레인지로버_4세대_2018_2022',\n",
       " '레인지로버_5세대_2023_2024',\n",
       " '레인지로버_벨라_2018_2019',\n",
       " '레인지로버_스포츠_2세대_2013_2017',\n",
       " '레인지로버_스포츠_2세대_2018_2022',\n",
       " '레인지로버_이보크_2016_2019',\n",
       " '레인지로버_이보크_2세대_2020_2022',\n",
       " '레인지로버_이보크_2세대_2023_2024',\n",
       " '렉스턴_스포츠_2018_2021',\n",
       " '렉스턴_스포츠_칸_2019_2020',\n",
       " '르반떼_2017_2022',\n",
       " '리얼_뉴_콜로라도_2021_2022',\n",
       " '마칸_2014_2018',\n",
       " '마칸_2019_2021',\n",
       " '마칸_2022_2024',\n",
       " '말리부_2012_2016',\n",
       " '머스탱_2015_2023',\n",
       " '모닝_어반_JA_2021_2023',\n",
       " '모델_3_2019_2022',\n",
       " '모델_3_2024_2025',\n",
       " '모델_Y_2021_2025',\n",
       " '모하비_더_마스터_2020_2024',\n",
       " '몬데오_4세대_2015_2020',\n",
       " '박스터_718_2017_2024',\n",
       " '베뉴_2020_2024',\n",
       " '베리_뉴_티볼리_2020_2023',\n",
       " '벨로스터_JS_2018_2020',\n",
       " '뷰티풀_코란도_2019_2024',\n",
       " '셀토스_2020_2023',\n",
       " '스타리아_2022_2025',\n",
       " '스토닉_2018_2020',\n",
       " '스팅어_2018_2020',\n",
       " '스팅어_마이스터_2021_2023',\n",
       " '스파크_2012_2015',\n",
       " '스포티지_4세대_2016_2018',\n",
       " '스포티지_5세대_2022_2024',\n",
       " '스포티지_더_볼드_2019_2022',\n",
       " '시에나_4세대_2021_2024',\n",
       " '싼타페_MX5_2024_2025',\n",
       " '싼타페_TM_2019_2020',\n",
       " '싼타페_더_프라임_2016_2018',\n",
       " '쏘나타_DN8_2020_2023',\n",
       " '쏘나타_뉴_라이즈_2018_2019',\n",
       " '쏘나타_디_엣지_DN8_2024_2025',\n",
       " '쏘렌토_4세대_2021_2023',\n",
       " '아반떼_AD_2016_2018',\n",
       " '아반떼_CN7_2021_2023',\n",
       " '아반떼_MD_2011_2014',\n",
       " '아반떼_N_2022_2023',\n",
       " '아반떼_하이브리드_CN7_2021_2023',\n",
       " '아베오_2012_2016',\n",
       " '아슬란_2015_2018',\n",
       " '아이오닉5_2022_2023',\n",
       " '아이오닉6_2023_2025',\n",
       " '아이오닉_하이브리드_2016_2019',\n",
       " '아테온_2018_2023',\n",
       " '알티마_2017_2018',\n",
       " '액티언_2세대_2025',\n",
       " '어코드_10세대_2018_2022',\n",
       " '에비에이터_2세대_2020_2025',\n",
       " '에스컬레이드_2015_2020',\n",
       " '에스컬레이드_5세대_2021_2024',\n",
       " '에쿠스_신형_2010_2015',\n",
       " '엑센트_신형_2011_2019',\n",
       " '올_뉴_K3_2019_2021',\n",
       " '올_뉴_K7_2016_2019',\n",
       " '올_뉴_K7_하이브리드_2017_2019',\n",
       " '올_뉴_렉스턴_2021_2025',\n",
       " '올_뉴_말리부_2017_2018',\n",
       " '올_뉴_모닝_2012_2015',\n",
       " '올_뉴_모닝_JA_2017_2020',\n",
       " '올_뉴_쏘렌토_2015_2017',\n",
       " '올_뉴_카니발_2015_2019',\n",
       " '올_뉴_카마로_2017_2018',\n",
       " '올_뉴_투싼_TL_2016_2018',\n",
       " '올_뉴_투싼_TL_2019_2020',\n",
       " '올란도_2012_2018',\n",
       " '익스플로러_2016_2017',\n",
       " '익스플로러_2018_2019',\n",
       " '익스플로러_6세대_2020_2025',\n",
       " '일렉트리파이드_GV70_2022_2024',\n",
       " '임팔라_2016_2019',\n",
       " '제네시스_DH_2014_2016',\n",
       " '체로키_KL_2019_2023',\n",
       " '카니발_4세대_2021',\n",
       " '카니발_4세대_2022_2023',\n",
       " '카이엔_PO536_2019_2023',\n",
       " '캐스퍼_2022_2024',\n",
       " '캠리_XV70_2018_2024',\n",
       " '컨티넨탈_10세대_2017_2019',\n",
       " '컨티넨탈_GT_2세대_2012_2017',\n",
       " '컨티넨탈_GT_3세대_2018_2023',\n",
       " '컴패스_2세대_2018_2022',\n",
       " '코나_2018_2020',\n",
       " '코나_SX2_2023_2025',\n",
       " '코세어_2020_2022',\n",
       " '콜로라도_2020_2020',\n",
       " '콰트로포르테_2014_2016',\n",
       " '콰트로포르테_2017_2022',\n",
       " '쿠퍼_컨버터블_2016_2024',\n",
       " '쿠퍼_컨트리맨_2012_2015',\n",
       " '쿠퍼_컨트리맨_2016_2024',\n",
       " '쿠퍼_클럽맨_2016_2024',\n",
       " '타이칸_2021_2025',\n",
       " '토레스_2023_2025',\n",
       " '투싼_NX4_2021_2023',\n",
       " '투아렉_3세대_2020_2023',\n",
       " '트래버스_2020_2023',\n",
       " '트랙스_2013_2016',\n",
       " '트랙스_크로스오버_2024_2025',\n",
       " '트레일블레이저_2021_2022',\n",
       " '트레일블레이저_2023',\n",
       " '티구안_올스페이스_2018_2023',\n",
       " '티볼리_2015_2018',\n",
       " '티볼리_아머_2018_2019',\n",
       " '티볼리_에어_2016_2019',\n",
       " '티볼리_에어_2021_2022',\n",
       " '파나메라_2010_2016',\n",
       " '파나메라_971_2017_2023',\n",
       " '파사트_GT_B8_2018_2022',\n",
       " '파일럿_3세대_2016_2018',\n",
       " '팰리세이드_2019_2022',\n",
       " '팰리세이드_LX3_2025',\n",
       " '프리우스_4세대_2016_2018',\n",
       " '프리우스_4세대_2019_2022',\n",
       " '프리우스_C_2018_2020']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lebel = sorted(list(set([p.split('/')[0] for p in train_list])))\n",
    "train_lebel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2368790/819159688.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  premodel.load_state_dict(torch.load(f\"cam_pth_b3.pth\", map_location=device))\n",
      "/tmp/ipykernel_2368790/3160409408.py:3: UserWarning: Argument(s) 'always_apply' are not valid for transform BasicTransform\n",
      "  super().__init__(always_apply=False, p=p)\n"
     ]
    }
   ],
   "source": [
    "stratify = [i.split(\"/\")[0] for i in train_list]\n",
    "train_root, val_root = train_test_split(\n",
    "    train_list,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=stratify\n",
    ")\n",
    "\n",
    "num_classes = len(train_lebel)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"efficientnet_b3\"\n",
    "premodel = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n",
    "premodel.load_state_dict(torch.load(f\"cam_pth_b3.pth\", map_location=device))\n",
    "\n",
    "target_layer = premodel.blocks[-1][-1]  # Swin에서는 이 부분이 GAP 후 FC 전, CAM 추출에 적합\n",
    "\n",
    "cam_crop = CamAttentionCrop(\n",
    "    model=premodel, \n",
    "    target_layer=target_layer, \n",
    "    size=(448, 448),\n",
    "    device=device,\n",
    "    p=1.0\n",
    ")\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    HalfCrop(mode='random', p=0.3),\n",
    "    # ✅ 정보 손실을 줄이고, 세부 crop 보존\n",
    "    A.RandomResizedCrop(\n",
    "        size=(448, 448),\n",
    "        scale=(0.5, 1.0),       # 50~100% 면적\n",
    "        ratio=(0.75, 1.33),     # 더 다양한 aspect ratio\n",
    "        p=1.0\n",
    "    ),\n",
    "    \n",
    "    # A.Resize(448,448),\n",
    "    # # ✅ 너무 센 augment 제거하고, 디테일 보존 위주로 선택\n",
    "    A.SomeOf([\n",
    "        A.HorizontalFlip(p=1.0),  # 대칭만 유지\n",
    "        A.RandomBrightnessContrast(p=1.0),  # 전체 명도 대비 조정\n",
    "        A.HueSaturationValue(p=1.0),  # 색조 변경 (너무 세지 않게)\n",
    "        A.CLAHE(p=1.0),  # 국소 대비 강화 → fine-grained에 도움\n",
    "        A.OneOf([\n",
    "            A.Sharpen(alpha=(0.1, 0.3)),  # Fine edge 강조\n",
    "            A.GaussianBlur(blur_limit=(3, 5))  # 흐린 이미지 일반화\n",
    "        ], p=1.0)\n",
    "    ], n=2, replace=False, p=0.6),  # 2개만 선택\n",
    "\n",
    "    CutoutCustom(size=100, p=0.3),  # ✅ 사용자 정의 Cutout\n",
    "    BlockMasking(num_blocks=15, block_size=15, p=0.3),  # ✅ 사용자 정의 블록 마스킹\n",
    "    \n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.RandomResizedCrop(\n",
    "        size=(448, 448),\n",
    "        scale=(0.5, 1.0),       # 50~100% 면적\n",
    "        ratio=(0.75, 1.33),     # 더 다양한 aspect ratio\n",
    "        p=1.0\n",
    "    ),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LabelEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_lebel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ConcatDataset([CustomDataset(train_path, train_root, 'train', train_transform, label_encoder)])# , CustomDataset(train_path, train_root, 'train', test_transform, label_encoder)])\n",
    "boosting_dataset = ConcatDataset([CustomDataset(train_path, train_root, 'train', test_transform, label_encoder), CustomDataset(train_path, val_root, 'train', test_transform, label_encoder)])\n",
    "val_dataset = CustomDataset(train_path, val_root, 'train', test_transform, label_encoder)\n",
    "test_dataset = CustomDataset(test_path, test_list, 'test', transform=None, label_encoder=label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. 모델 준비\n",
    "# premodel.to(device)\n",
    "# premodel.eval()\n",
    "\n",
    "# # 2. CAM extractor 준비\n",
    "# cam_extractor = SmoothGradCAMpp(premodel, target_layer=target_layer)  # Swin에서 'norm' 사용\n",
    "\n",
    "# # 3. 데이터 하나 가져오기\n",
    "# image_tensor, label = val_dataset[0]  # image_tensor: (C, H, W)\n",
    "\n",
    "# # 4. CAM 추출\n",
    "# input_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "# output = premodel(input_tensor)\n",
    "# pred = output.argmax(dim=1).item()\n",
    "# activation_map = cam_extractor(pred, output)[0].detach()  # (H, W)\n",
    "\n",
    "\n",
    "# # 5. 이미지 복원 (normalize 해제)\n",
    "# image_np = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "# image_np = (image_np * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]\n",
    "# image_np = (image_np * 255).astype('uint8')\n",
    "\n",
    "# # 6. CAM 색상 맵 만들기\n",
    "# # 1. 토큰 임베딩에서 spatial map으로 reshape\n",
    "# # 예: (1, 12, 1024) → (1, 3, 4, 1024) → (3, 4)\n",
    "# activation_map = activation_map.squeeze(0)  # (12, 1024)\n",
    "\n",
    "# # 2. 각 위치별로 L2 norm 또는 mean으로 summary\n",
    "# activation_map = activation_map.norm(dim=-1)  # (12,) 또는 .mean(dim=-1)\n",
    "\n",
    "# # 3. spatial 형태로 reshape (예: 3 x 4)\n",
    "# activation_map = activation_map.reshape(3, 4)  # or 적절한 H, W 값 (patch 수 기반)\n",
    "\n",
    "# # 4. resize to image shape for CAM\n",
    "# activation_map = cv2.resize(activation_map.cpu().numpy(), (image_np.shape[1], image_np.shape[0]))\n",
    "# activation_map = np.clip(activation_map, 0, 1)\n",
    "# activation_map = np.uint8(255 * activation_map)\n",
    "\n",
    "# # 5. apply colormap\n",
    "# heatmap = cv2.applyColorMap(activation_map, cv2.COLORMAP_JET)\n",
    "# heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "# heatmap = cv2.resize(heatmap, (image_np.shape[1], image_np.shape[0]))\n",
    "\n",
    "# # 6. overlay\n",
    "# cam_overlay = cv2.addWeighted(image_np, 0.5, heatmap, 0.5, 0)\n",
    "\n",
    "# # 7. 시각화\n",
    "# plt.figure(figsize=(12, 4))\n",
    "# plt.subplot(1, 3, 1)\n",
    "# plt.imshow(image_np)\n",
    "# plt.title(\"Original Image\")\n",
    "# plt.axis('off')\n",
    "\n",
    "# plt.subplot(1, 3, 2)\n",
    "# plt.imshow(activation_map, cmap='jet')\n",
    "# plt.title(\"CAM Map\")\n",
    "# plt.axis('off')\n",
    "\n",
    "# plt.subplot(1, 3, 3)\n",
    "# plt.imshow(cam_overlay)\n",
    "# plt.title(\"Overlay\")\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_workers = 8\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "boosting_dataloader = DataLoader(\n",
    "    boosting_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFace(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # Normalize input and weights\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))  # cosine(theta)\n",
    "\n",
    "        if label is None:\n",
    "            return self.s * cosine  # inference 시 margin 없이 리턴\n",
    "\n",
    "        # Clamp to valid range\n",
    "        theta = torch.acos(torch.clamp(cosine, -1.0 + 1e-7, 1.0 - 1e-7))\n",
    "        target_logits = torch.cos(theta + self.m)\n",
    "\n",
    "        # 🔧 안정성을 위한 보강\n",
    "        label = label.long().view(-1, 1)\n",
    "        one_hot = torch.zeros_like(cosine, device=cosine.device)\n",
    "        one_hot.scatter_(1, label, 1.0)\n",
    "\n",
    "        output = self.s * (one_hot * target_logits + (1.0 - one_hot) * cosine)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArcFace(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, s=30.0, m=0.50):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)  # classifier 제거\n",
    "        self.backbone.head = nn.Identity()\n",
    "        self.feature_dim = self.backbone.num_features\n",
    "        self.arcface = ArcFace(self.feature_dim, num_classes, s=s, m=m)\n",
    "        self.softmax = nn.Linear(self.feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        # Swin 계열은 forward_features 사용\n",
    "        if hasattr(self.backbone, 'forward_features'):\n",
    "            features = self.backbone.forward_features(x)\n",
    "        else:\n",
    "            features = self.backbone(x)\n",
    "\n",
    "        features = features.mean(dim=(1, 2))\n",
    "            \n",
    "        arc = self.arcface(features, label)\n",
    "        softmax = self.softmax(features)\n",
    "\n",
    "        return arc, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647369228/work/aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eva(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (rope): RotaryEmbeddingCat()\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x EvaBlock(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): EvaAttention(\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): SwiGLU(\n",
      "        (fc1_g): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (fc1_x): Linear(in_features=768, out_features=2048, bias=True)\n",
      "        (act): SiLU()\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): Identity()\n",
      "  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (head_drop): Dropout(p=0.0, inplace=False)\n",
      "  (head): Linear(in_features=768, out_features=396, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(train_lebel)  # 라벨 개수\n",
    "model_name = \"eva02_base_patch14_448\"\n",
    "# model = MultiArcFace(model_name, num_classes)\n",
    "model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "base_optimizer = torch.optim.AdamW\n",
    "optimizer = SAM(model.parameters(), base_optimizer, weight_decay=2.e-7, lr=5.e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "swa_model = AveragedModel(model)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer.base_optimizer, T_max=150)\n",
    "# scheduler = SWALR(optimizer, swa_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrated_logits(logits, T=2.0):\n",
    "    return logits / T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\", total=len(dataloader)):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            # outputs, _ = model(images, labels)  # ArcFace requires labels\n",
    "            outputs = model(images)  \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        loss = closure()\n",
    "        optimizer.step(closure)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def val_one_epoch(model, dataloader, device, criterion, num_classes):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    wrong_answer = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validating\", total=len(dataloader)):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # outputs, _ = model(images, labels)  # 이제 label 없어도 OK\n",
    "            outputs = model(images)\n",
    "            # outputs = calibrated_logits(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            if preds != labels:\n",
    "                wrong_answer.append((images, labels, preds))\n",
    "\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total * 100\n",
    "    val_logloss = log_loss(all_labels, all_probs, labels=list(range(num_classes)))\n",
    "\n",
    "    return average_loss, accuracy, val_logloss, wrong_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1657 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1657/1657 [47:56<00:00,  1.74s/it]\n",
      "Validating: 100%|██████████| 6628/6628 [02:41<00:00, 41.10it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 4.299, val acc : 64.94, val loss : 1.55, val logloss : 1.55\n",
      "📦 Best model saved at epoch 1 (logloss: 1.5498)\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1657/1657 [48:00<00:00,  1.74s/it]\n",
      "Validating: 100%|██████████| 6628/6628 [02:40<00:00, 41.35it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.7783, val acc : 88.94, val loss : 0.4192, val logloss : 0.4192\n",
      "📦 Best model saved at epoch 2 (logloss: 0.4192)\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1657/1657 [48:00<00:00,  1.74s/it]\n",
      "Validating: 100%|██████████| 6628/6628 [02:40<00:00, 41.40it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.2938, val acc : 91.57, val loss : 0.2831, val logloss : 0.2831\n",
      "📦 Best model saved at epoch 3 (logloss: 0.2831)\n",
      "epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1657/1657 [47:58<00:00,  1.74s/it]\n",
      "Validating: 100%|██████████| 6628/6628 [02:40<00:00, 41.40it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.1936, val acc : 92.97, val loss : 0.2347, val logloss : 0.2347\n",
      "📦 Best model saved at epoch 4 (logloss: 0.2347)\n",
      "epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1657/1657 [47:58<00:00,  1.74s/it]\n",
      "Validating: 100%|██████████| 6628/6628 [02:40<00:00, 41.41it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.152, val acc : 94.25, val loss : 0.1925, val logloss : 0.1925\n",
      "📦 Best model saved at epoch 5 (logloss: 0.1925)\n",
      "epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1657/1657 [48:01<00:00,  1.74s/it]\n",
      "Validating: 100%|██████████| 6628/6628 [02:40<00:00, 41.36it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.1248, val acc : 94.79, val loss : 0.1851, val logloss : 0.1851\n",
      "📦 Best model saved at epoch 6 (logloss: 0.1851)\n",
      "epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1657/1657 [48:01<00:00,  1.74s/it]\n",
      "Validating: 100%|██████████| 6628/6628 [02:40<00:00, 41.35it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.1076, val acc : 94.72, val loss : 0.172, val logloss : 0.172\n",
      "📦 Best model saved at epoch 7 (logloss: 0.1720)\n",
      "epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1657/1657 [48:00<00:00,  1.74s/it]\n",
      "Validating: 100%|██████████| 6628/6628 [02:40<00:00, 41.39it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.09674, val acc : 95.22, val loss : 0.1537, val logloss : 0.1537\n",
      "📦 Best model saved at epoch 8 (logloss: 0.1537)\n",
      "epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1657/1657 [48:00<00:00,  1.74s/it]\n",
      "Validating: 100%|██████████| 6628/6628 [02:40<00:00, 41.34it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.08491, val acc : 95.5, val loss : 0.1496, val logloss : 0.1496\n",
      "📦 Best model saved at epoch 9 (logloss: 0.1496)\n",
      "epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1657/1657 [48:00<00:00,  1.74s/it]\n",
      "Validating: 100%|██████████| 6628/6628 [02:40<00:00, 41.40it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.07759, val acc : 95.75, val loss : 0.1467, val logloss : 0.1467\n",
      "📦 Best model saved at epoch 10 (logloss: 0.1467)\n",
      "epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1657/1657 [48:01<00:00,  1.74s/it]\n",
      "Validating: 100%|██████████| 6628/6628 [02:40<00:00, 41.42it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.07305, val acc : 95.56, val loss : 0.1552, val logloss : 0.1552\n",
      "epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1657/1657 [48:00<00:00,  1.74s/it]\n",
      "Validating: 100%|██████████| 6628/6628 [02:40<00:00, 41.39it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.06578, val acc : 95.59, val loss : 0.1521, val logloss : 0.1521\n",
      "epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1657/1657 [48:01<00:00,  1.74s/it]\n",
      "Validating: 100%|██████████| 6628/6628 [02:40<00:00, 41.36it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.05929, val acc : 95.78, val loss : 0.1356, val logloss : 0.1356\n",
      "📦 Best model saved at epoch 13 (logloss: 0.1356)\n",
      "epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1657/1657 [48:01<00:00,  1.74s/it]\n",
      "Validating: 100%|██████████| 6628/6628 [02:40<00:00, 41.41it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.05571, val acc : 96.33, val loss : 0.1299, val logloss : 0.1299\n",
      "📦 Best model saved at epoch 14 (logloss: 0.1299)\n",
      "epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1657/1657 [48:02<00:00,  1.74s/it]\n",
      "Validating: 100%|██████████| 6628/6628 [02:40<00:00, 41.37it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 0.05178, val acc : 95.75, val loss : 0.1471, val logloss : 0.1471\n",
      "epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████    | 1004/1657 [29:05<18:55,  1.74s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     val_loss, val_metric, val_logloss, wrong_answer \u001b[38;5;241m=\u001b[39m val_one_epoch(model, val_dataloader, device, criterion, num_classes)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val acc : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metric\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val logloss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_logloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep(closure)\n\u001b[1;32m     18\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m, in \u001b[0;36mtrain_one_epoch.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)  \n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/construct/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/construct/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/construct/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_metric = float('inf')\n",
    "best_logloss = float('inf')\n",
    "current_patient = 0\n",
    "patient = 5\n",
    "epochs = 50\n",
    "swa_start = int(0.75 * epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print(f\"epoch {epoch+1}\")\n",
    "    train_loss = train_one_epoch(model, train_dataloader, optimizer, criterion, device)\n",
    "    val_loss, val_metric, val_logloss, wrong_answer = val_one_epoch(model, val_dataloader, device, criterion, num_classes)\n",
    "\n",
    "    print(f\"train loss : {train_loss:.4}, val acc : {val_metric:.4}, val loss : {val_loss:.4}, val logloss : {val_logloss:.4}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # if epoch > swa_start:\n",
    "    #     swa_model.update_parameters(model)\n",
    "    #     scheduler.step()\n",
    "    # else:\n",
    "    #     scheduler.step()\n",
    "\n",
    "    # Best model 저장\n",
    "    if val_logloss < best_logloss:\n",
    "        best_logloss = val_logloss\n",
    "        torch.save(model.state_dict(), f'best_model.pth')\n",
    "        print(f\"📦 Best model saved at epoch {epoch+1} (logloss: {val_logloss:.4f})\")\n",
    "        current_patient = 0\n",
    "        best_val_metric = val_loss\n",
    "    else:\n",
    "        current_patient += 1\n",
    "        if current_patient > patient:\n",
    "            print(f\"early stopping!! at Epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# update_bn(train_dataloader, swa_model)\n",
    "# torch.save(swa_model.module.state_dict(), \"swa_best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2368790/2605037189.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/33137 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 33137/33137 [13:21<00:00, 41.34it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Wrong samples: 627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "boosting: 100%|██████████| 40/40 [01:07<00:00,  1.69s/it]\n",
      "boosting: 100%|██████████| 40/40 [01:07<00:00,  1.70s/it]\n",
      "boosting: 100%|██████████| 40/40 [01:07<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 33137/33137 [13:27<00:00, 41.04it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Wrong samples: 565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "boosting: 100%|██████████| 36/36 [01:00<00:00,  1.69s/it]\n",
      "boosting: 100%|██████████| 36/36 [01:01<00:00,  1.70s/it]\n",
      "boosting: 100%|██████████| 36/36 [01:01<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 33137/33137 [13:27<00:00, 41.05it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Wrong samples: 511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "boosting: 100%|██████████| 32/32 [00:54<00:00,  1.71s/it]\n",
      "boosting: 100%|██████████| 32/32 [00:55<00:00,  1.73s/it]\n",
      "boosting: 100%|██████████| 32/32 [00:55<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "# 1. 모델 불러오기\n",
    "model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# 반복 조건\n",
    "max_iter = 3\n",
    "threshold = 50\n",
    "current_iter = 0\n",
    "\n",
    "num_epochs_per_boost = 3\n",
    "\n",
    "# criterion, optimizer 정의 (학습용)\n",
    "base_optimizer = torch.optim.AdamW\n",
    "optimizer = SAM(model.parameters(), base_optimizer, weight_decay=2.e-7, lr=1.e-6)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer.base_optimizer, T_max=20)\n",
    "\n",
    "while current_iter < max_iter:\n",
    "    print(f\"\\n🔁 Iteration {current_iter + 1}\")\n",
    "\n",
    "    # 2. 전체 train dataset 에서 잘못 맞춘 샘플 추출\n",
    "    _, _, _, wrong_answer = val_one_epoch(model, boosting_dataloader, device, criterion, num_classes)\n",
    "\n",
    "    print(f\"❌ Wrong samples: {len(wrong_answer)}\")\n",
    "    if len(wrong_answer) <= threshold:\n",
    "        print(\"✅ Enough correction, stopping early.\")\n",
    "        break\n",
    "\n",
    "    # 3. 틀린 샘플만을 이용한 DataLoader 생성\n",
    "    images, labels, _ = zip(*wrong_answer)  # image: Tensor (C, H, W), label: int\n",
    "    images = torch.stack(images)            # (N, C, H, W)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    images = images.squeeze(1)\n",
    "\n",
    "    wrong_dataset = TensorDataset(images, labels)\n",
    "    wrong_loader = DataLoader(wrong_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    # 4. 틀린 샘플만을 이용한 재학습\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs_per_boost):\n",
    "        for images, labels in tqdm(wrong_loader, desc=\"boosting\", total=len(wrong_loader)):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # images = train_transform(image=images)[\"image\"]\n",
    "\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)  \n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            loss = closure()\n",
    "            optimizer.step(closure)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    current_iter += 1\n",
    "\n",
    "    torch.save(model.state_dict(), f'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2368790/2296973913.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"best_model.pth\", map_location=device))\n",
      "Validating:   2%|▏         | 120/6628 [00:02<02:37, 41.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 6628/6628 [02:41<00:00, 41.06it/s]\n",
      "/home/aicontest/anaconda3/envs/construct/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f\"best_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "val_loss, val_metric, val_logloss, wrong_answer = val_one_epoch(model, val_dataloader, device, criterion, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "mean = torch.tensor(mean).view(-1, 1, 1).to(device)\n",
    "std = torch.tensor(std).view(-1, 1, 1).to(device)\n",
    "\n",
    "print(f\"wrong answer : {len(wrong_answer)}\")\n",
    "\n",
    "for i, l, w in wrong_answer[:20]:\n",
    "    image = (i * std + mean).squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    label = label_encoder.inverse_transform([l.cpu()])\n",
    "    wrong_label = label_encoder.inverse_transform([w.cpu()])\n",
    "\n",
    "    print(f\"{label} - {wrong_label}\")\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tta_random_crop_batch(model, image_np_batch, transform, device, tta_times=3):\n",
    "    model.eval()\n",
    "    batch_size = len(image_np_batch)\n",
    "    all_probs = [[] for _ in range(batch_size)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(tta_times):\n",
    "            aug_batch = []\n",
    "\n",
    "            # TTA 변형 반복 적용\n",
    "            for image_np in image_np_batch:\n",
    "                aug = transform(image=image_np)[\"image\"]  # Tensor (C, H, W)\n",
    "                aug_batch.append(aug)\n",
    "\n",
    "            aug_batch = torch.stack(aug_batch).to(device)  # (B, C, H, W)\n",
    "\n",
    "            logits = model(aug_batch)  # (B, num_classes)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                all_probs[i].append(probs[i])\n",
    "\n",
    "    # 각 이미지에 대해 confidence(softmax max값)가 가장 높은 결과 선택\n",
    "    max_confidence_probs = []\n",
    "    for prob_list in all_probs:\n",
    "        # 각 TTA 결과의 confidence 추출\n",
    "        confidences = [prob.max().item() for prob in prob_list]\n",
    "        best_idx = confidences.index(max(confidences))\n",
    "        max_confidence_probs.append(prob_list[best_idx])\n",
    "\n",
    "    max_confidence_probs = torch.stack(max_confidence_probs, dim=0)  # (B, num_classes)\n",
    "    return max_confidence_probs\n",
    "\n",
    "\n",
    "    # # 평균 계산\n",
    "    # avg_probs = [torch.stack(p_list, dim=0).mean(dim=0) for p_list in all_probs]\n",
    "    # avg_probs = torch.stack(avg_probs, dim=0)  # (B, num_classes)\n",
    "    # return avg_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2368790/954877336.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"best_model.pth\", map_location=device))\n",
      "TTA Test: 100%|██████████| 8258/8258 [14:59<00:00,  9.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# 저장된 모델 로드\n",
    "# model = ConvNeXtArcFace(model_name=model_name, num_classes=num_classes)\n",
    "# model = MultiArcFace(model_name, num_classes)\n",
    "model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(f\"best_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "temperature = 1\n",
    "\n",
    "model.eval()\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in tqdm(test_dataloader, desc=\"TTA Test\", total=len(test_dataloader)):\n",
    "        image_np = images.numpy()  # (H, W, C) 형태\n",
    "        # images = images.to(device)\n",
    "\n",
    "        probs = tta_random_crop_batch(model, image_np, test_transform, device, tta_times=5)\n",
    "        # probs = model(images)\n",
    "        probs = probs / temperature\n",
    "        probs = F.softmax(probs, dim=1)\n",
    "\n",
    "        for prob in probs.cpu():\n",
    "            result = {\n",
    "                train_lebel[i]: prob[i].item()\n",
    "                for i in range(len(train_lebel))\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "\n",
    "# 결과 정리\n",
    "pred = pd.DataFrame(results)\n",
    "sample_submission = pd.read_csv(\"/home/aicontest/HAI_car/data/sample_submission.csv\")\n",
    "class_columns = sample_submission.columns[1:]\n",
    "pred = pred[class_columns]\n",
    "sample_submission[class_columns] = pred.values\n",
    "sample_submission.to_csv('submission.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compare_rowwise_probability_diff(csv1_path, csv2_path, top_k=10, only_different_preds=False):\n",
    "    df1 = pd.read_csv(csv1_path)\n",
    "    df2 = pd.read_csv(csv2_path)\n",
    "\n",
    "    assert df1.shape == df2.shape, \"Shape mismatch between CSVs\"\n",
    "\n",
    "    # ID 제외한 확률 컬럼\n",
    "    exclude_cols = ['ID'] if 'ID' in df1.columns else []\n",
    "    class_cols = [col for col in df1.columns if col not in exclude_cols]\n",
    "\n",
    "    diffs = []\n",
    "\n",
    "    num_wrong = 0\n",
    "    for i in range(len(df1)):\n",
    "        probs1 = df1[class_cols].iloc[i].values.astype(float)\n",
    "        probs2 = df2[class_cols].iloc[i].values.astype(float)\n",
    "\n",
    "        pred1 = np.argmax(probs1)\n",
    "        pred2 = np.argmax(probs2)\n",
    "\n",
    "        prob_diff = np.abs(probs1 - probs2).mean()\n",
    "        sample_id = df1['ID'].iloc[i] if 'ID' in df1.columns else i\n",
    "        if pred1 != pred2:\n",
    "            num_wrong += 1\n",
    "            diffs.append((sample_id, pred1, pred2, prob_diff))\n",
    "\n",
    "    # 차이 큰 순서대로 정렬\n",
    "    sorted_diffs = sorted(diffs, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "    print(f\"wrong : {num_wrong}\")\n",
    "    print(f\"Top {top_k} rows with largest mean probability difference\" +\n",
    "          (\" (only different predictions)\" if only_different_preds else \"\") + \":\")\n",
    "\n",
    "    for rank, (idx, p1, p2, d) in enumerate(sorted_diffs[:top_k], 1):\n",
    "        print(f\"{rank:2d}. ID: {idx}, Pred1: {p1}, Pred2: {p2}, MeanAbsDiff: {d:.4f}\")\n",
    "\n",
    "    return pd.DataFrame(sorted_diffs, columns=[\"ID\", \"Pred1\", \"Pred2\", \"MeanAbsDiff\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong : 173\n",
      "Top 20 rows with largest mean probability difference:\n",
      " 1. ID: TEST_07982, Pred1: 380, Pred2: 381, MeanAbsDiff: 0.0050\n",
      " 2. ID: TEST_00722, Pred1: 356, Pred2: 283, MeanAbsDiff: 0.0050\n",
      " 3. ID: TEST_03770, Pred1: 156, Pred2: 155, MeanAbsDiff: 0.0050\n",
      " 4. ID: TEST_01029, Pred1: 181, Pred2: 368, MeanAbsDiff: 0.0049\n",
      " 5. ID: TEST_06988, Pred1: 352, Pred2: 71, MeanAbsDiff: 0.0049\n",
      " 6. ID: TEST_07015, Pred1: 370, Pred2: 371, MeanAbsDiff: 0.0049\n",
      " 7. ID: TEST_06600, Pred1: 269, Pred2: 270, MeanAbsDiff: 0.0049\n",
      " 8. ID: TEST_00670, Pred1: 42, Pred2: 77, MeanAbsDiff: 0.0049\n",
      " 9. ID: TEST_05734, Pred1: 25, Pred2: 26, MeanAbsDiff: 0.0049\n",
      "10. ID: TEST_00121, Pred1: 93, Pred2: 207, MeanAbsDiff: 0.0049\n",
      "11. ID: TEST_06347, Pred1: 24, Pred2: 23, MeanAbsDiff: 0.0049\n",
      "12. ID: TEST_05606, Pred1: 15, Pred2: 14, MeanAbsDiff: 0.0049\n",
      "13. ID: TEST_03542, Pred1: 366, Pred2: 281, MeanAbsDiff: 0.0049\n",
      "14. ID: TEST_03624, Pred1: 264, Pred2: 90, MeanAbsDiff: 0.0049\n",
      "15. ID: TEST_03196, Pred1: 23, Pred2: 24, MeanAbsDiff: 0.0049\n",
      "16. ID: TEST_02216, Pred1: 23, Pred2: 19, MeanAbsDiff: 0.0049\n",
      "17. ID: TEST_03312, Pred1: 239, Pred2: 233, MeanAbsDiff: 0.0049\n",
      "18. ID: TEST_00196, Pred1: 223, Pred2: 279, MeanAbsDiff: 0.0049\n",
      "19. ID: TEST_04365, Pred1: 262, Pred2: 253, MeanAbsDiff: 0.0049\n",
      "20. ID: TEST_02984, Pred1: 19, Pred2: 104, MeanAbsDiff: 0.0049\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Pred1</th>\n",
       "      <th>Pred2</th>\n",
       "      <th>MeanAbsDiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_07982</td>\n",
       "      <td>380</td>\n",
       "      <td>381</td>\n",
       "      <td>0.004993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00722</td>\n",
       "      <td>356</td>\n",
       "      <td>283</td>\n",
       "      <td>0.004969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_03770</td>\n",
       "      <td>156</td>\n",
       "      <td>155</td>\n",
       "      <td>0.004955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_01029</td>\n",
       "      <td>181</td>\n",
       "      <td>368</td>\n",
       "      <td>0.004945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_06988</td>\n",
       "      <td>352</td>\n",
       "      <td>71</td>\n",
       "      <td>0.004941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>TEST_00447</td>\n",
       "      <td>169</td>\n",
       "      <td>52</td>\n",
       "      <td>0.002762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>TEST_02243</td>\n",
       "      <td>149</td>\n",
       "      <td>86</td>\n",
       "      <td>0.002614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>TEST_00773</td>\n",
       "      <td>266</td>\n",
       "      <td>373</td>\n",
       "      <td>0.002612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>TEST_05034</td>\n",
       "      <td>227</td>\n",
       "      <td>149</td>\n",
       "      <td>0.002413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>TEST_03225</td>\n",
       "      <td>282</td>\n",
       "      <td>284</td>\n",
       "      <td>0.002341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  Pred1  Pred2  MeanAbsDiff\n",
       "0    TEST_07982    380    381     0.004993\n",
       "1    TEST_00722    356    283     0.004969\n",
       "2    TEST_03770    156    155     0.004955\n",
       "3    TEST_01029    181    368     0.004945\n",
       "4    TEST_06988    352     71     0.004941\n",
       "..          ...    ...    ...          ...\n",
       "168  TEST_00447    169     52     0.002762\n",
       "169  TEST_02243    149     86     0.002614\n",
       "170  TEST_00773    266    373     0.002612\n",
       "171  TEST_05034    227    149     0.002413\n",
       "172  TEST_03225    282    284     0.002341\n",
       "\n",
       "[173 rows x 4 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_rowwise_probability_diff(\"submission.csv\", \"eva_submission.csv\", top_k=20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "construct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
